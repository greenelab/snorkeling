{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Discriminator for Candidate Classification on the Sentence Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to train ML algorithms: Long Short Term Memory Neural Net (LSTM) and SparseLogisticRegression (SLR) for candidate classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUST RUN AT THE START OF EVERYTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the database for data extraction and load the Candidate subclass for the algorithms below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "username = \"danich1\"\n",
    "password = \"snorkel\"\n",
    "dbname = \"pubmeddb\"\n",
    "\n",
    "#Path subject to change for different os\n",
    "database_str = \"postgresql+psycopg2://{}:{}@/{}?host=/var/run/postgresql\".format(username, password, dbname)\n",
    "os.environ['SNORKELDB'] = database_str\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator, LabelAnnotator, load_marginals\n",
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel.learning import SparseLogisticRegression\n",
    "from snorkel.learning.disc_models.rnn import reRNN\n",
    "from snorkel.learning.utils import RandomSearch\n",
    "from snorkel.models import Candidate, FeatureKey, candidate_subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_type = \"dg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edge_type == \"dg\":\n",
    "    DiseaseGene = candidate_subclass('DiseaseGene', ['Disease', 'Gene'])\n",
    "elif edge_type == \"gg\":\n",
    "    GeneGene = candidate_subclass('GeneGene', ['Gene1', 'Gene2'])\n",
    "elif edge_type == \"cg\":\n",
    "    CompoundGene = candidate_subclass('CompoundGene', ['Compound', 'Gene'])\n",
    "elif edge_type == \"cd\":\n",
    "    CompoundDisease = candidate_subclass('CompoundDisease', ['Compound', 'Disease'])\n",
    "else:\n",
    "    print(\"Please pick a valid edge type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a LSTM Disc Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code trains an LSTM. An LSTM is a special type of recurrent nerual network that retains a memory of past values over period of time. ([Further explaination here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)). The problem with the code below is that sqlalchemy runs into an out of memory error on my computer during the preprocessing step. As a consequence we have to resort loading this data onto University of Pennsylvania's Performance Computing Cluster. The data that gets preprocessed is exported to a text file and then get shipped towards the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Training data to an External File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_df = pd.read_excel(\"data/sentence-labels.xlsx\")\n",
    "train_sentences_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_ids = train_sentences_df.candidate_id.astype(int).tolist()\n",
    "candidate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = (\n",
    "    session\n",
    "    .query(Candidate)\n",
    "    .filter(Candidate.id.in_(candidate_ids))\n",
    "    .all() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = reRNN(seed=100, n_threads=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for c in tqdm.tqdm(train_cands):\n",
    "    data, ends = lstm._preprocess_data([c], extend=True)\n",
    "    rows.append({\n",
    "        \"data_str\": \",\".join([str(x) for x in data[0]]),\n",
    "        \"ends\": ends[0],\n",
    "        \"candidate_id\": c.id\n",
    "    })\n",
    "train_data = pd.DataFrame(rows)\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(\n",
    "    train_data,\n",
    "    train_sentences_df[[\"candidate_id\", \"label\"]],\n",
    "    how='left'\n",
    ")\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"data/lstm/train_data.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the word dictionary to an External File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"data/lstm/train_word_dict.csv\", 'w') as f:\n",
    "    output = csv.DictWriter(f, fieldnames=[\"Key\", \"Value\"])\n",
    "    output.writeheader()\n",
    "    for key in tqdm.tqdm(lstm.word_dict.d):\n",
    "        output.writerow({'Key':key, 'Value': lstm.word_dict.d[key]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Dev data to an External File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences_df = pd.read_excel(\"data/sentence-labels-dev.xlsx\")\n",
    "dev_sentences_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_ids = dev_sentences_df.candidate_id.astype(int).tolist()\n",
    "candidate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cands = (\n",
    "    session\n",
    "    .query(Candidate)\n",
    "    .filter(Candidate.id.in_(candidate_ids))\n",
    "    .all() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for c in tqdm.tqdm(dev_cands):\n",
    "    data, ends = lstm._preprocess_data([c])\n",
    "    rows.append({\n",
    "        \"data_str\": \",\".join([str(x) for x in data[0]]),\n",
    "        \"ends\": ends[0],\n",
    "        \"candidate_id\": c.id\n",
    "    })\n",
    "dev_data = pd.DataFrame(rows)\n",
    "dev_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = pd.merge(\n",
    "    dev_data,\n",
    "    dev_sentences_df[[\"candidate_id\", \"label\"]],\n",
    "    how='left'\n",
    ")\n",
    "dev_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data.to_csv(\"data/lstm/dev_data.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sparse Logistic Regression Disc Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train an SLR. To find the optimal hyperparameter settings this code uses a [random search](http://scikit-learn.org/stable/modules/grid_search.html) instead of iterating over all possible combinations of parameters. After the final model has been found, it is saved in the checkpoints folder to be loaded in the [next notebook](5.data-analysis.ipynb). Furthermore, the weights for the final model are output into a text file to be analyzed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_df = pd.read_excel(\"data/sentence-labels.xlsx\")\n",
    "train_sentences_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(\n",
    "    train_sentences_df.sentence.values\n",
    ")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    train_sentences_df.label.apply(lambda x: 1 if x > 0.5 else 0)\n",
    "]\n",
    "\n",
    "model_labels = [\n",
    "    \"all_LF_LR\"\n",
    "]\n",
    "    \n",
    "lr_grids = [\n",
    "    {'C':np.linspace(1,10, num=4)} for _ in range(len(labels))\n",
    "]\n",
    "final_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for grid, y_labels in zip(lr_grids, labels):\n",
    "    fit_model = GridSearchCV(lr_model, \n",
    "                         grid, cv=10, n_jobs=3, \n",
    "                         verbose=1, scoring='roc_auc', return_train_score=True)\n",
    "    fit_model.fit(X, y_labels)\n",
    "    final_models.append(fit_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for model, model_label in zip(final_models, model_labels):\n",
    "    lr_result = pd.DataFrame(model.cv_results_)\n",
    "    plt.plot(lr_result[\"param_C\"], lr_result[\"mean_test_score\"], label=model_label)\n",
    "plt.legend()\n",
    "plt.xlabel(\"C (regularization parameter)\")\n",
    "plt.ylabel(\"Mean Test Score\")\n",
    "plt.title(\"BOW Training CV (10-fold)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, label in zip(range(len(final_models)), model_labels):\n",
    "    lr_weights = pd.DataFrame(list(zip(final_models[i].best_estimator_.coef_[0], vectorizer.get_feature_names())), columns=[\"Weight\", \"Feature\"])\n",
    "    print(label)\n",
    "    print(lr_weights.sort_values(\"Weight\", ascending=False).head(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_marginals = []\n",
    "for model in final_models:\n",
    "    lr_marginals.append(model.best_estimator_.predict_proba(dev_X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(final_models[0].best_estimator_.predict(dev_X)).value_counts())\n",
    "print()\n",
    "print(pd.Series(final_models[1].best_estimator_.predict(dev_X)).value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for marginal, model_label in zip(lr_marginals,model_labels):\n",
    "    filename = \"vanilla_lstm/lstm_disease_gene_holdout/subsampled/lf_marginals/{}_dev_marginals.csv\".format(model_label)\n",
    "    pd.DataFrame(marginal,\n",
    "             columns=[\"LR_Marginals\"]\n",
    "        ).to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
