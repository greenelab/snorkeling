{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Discriminator for Candidate Classification on the Sentence Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to train the following machine learning models: Long Short Term Memory Network (LSTM), [Doc2Vec](https://arxiv.org/pdf/1707.02377.pdf) with Logitstic Regression, and [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) with Logistic Regrssion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUST RUN AT THE START OF EVERYTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the database for data extraction and load the Candidate subclass for the algorithms below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:26:58.094377Z",
     "start_time": "2018-08-13T15:26:53.534408Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:26:59.027005Z",
     "start_time": "2018-08-13T15:26:58.098484Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "username = \"danich1\"\n",
    "password = \"snorkel\"\n",
    "dbname = \"pubmeddb\"\n",
    "\n",
    "#Path subject to change for different os\n",
    "database_str = \"postgresql+psycopg2://{}:{}@/{}?host=/var/run/postgresql\".format(username, password, dbname)\n",
    "os.environ['SNORKELDB'] = database_str\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:27:02.671131Z",
     "start_time": "2018-08-13T15:26:59.028611Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator, load_marginals\n",
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel.learning.pytorch import LSTM\n",
    "from snorkel.models import Candidate, FeatureKey, candidate_subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:27:02.689794Z",
     "start_time": "2018-08-13T15:27:02.672799Z"
    }
   },
   "outputs": [],
   "source": [
    "edge_type = \"dg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:27:02.788664Z",
     "start_time": "2018-08-13T15:27:02.691313Z"
    }
   },
   "outputs": [],
   "source": [
    "if edge_type == \"dg\":\n",
    "    DiseaseGene = candidate_subclass('DiseaseGene', ['Disease', 'Gene'])\n",
    "elif edge_type == \"gg\":\n",
    "    GeneGene = candidate_subclass('GeneGene', ['Gene1', 'Gene2'])\n",
    "elif edge_type == \"cg\":\n",
    "    CompoundGene = candidate_subclass('CompoundGene', ['Compound', 'Gene'])\n",
    "elif edge_type == \"cd\":\n",
    "    CompoundDisease = candidate_subclass('CompoundDisease', ['Compound', 'Disease'])\n",
    "else:\n",
    "    print(\"Please pick a valid edge type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the sentences from the training and development set respectively. Both come from excel files that are on the repository as we speak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:28:05.098653Z",
     "start_time": "2018-08-13T15:27:02.791744Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sentences_df = pd.read_excel(\"data/disease_gene/sentence-labels.xlsx\")\n",
    "train_sentences_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:28:07.057591Z",
     "start_time": "2018-08-13T15:28:05.100578Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_sentences_df = pd.read_excel(\"data/sentence-labels-dev.xlsx\")\n",
    "dev_sentences_df = dev_sentences_df[dev_sentences_df.curated_dsh.notnull()]\n",
    "dev_sentences_df = dev_sentences_df.sort_values(\"candidate_id\")\n",
    "dev_sentences_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2VecC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code runs the doc2vec model, which is a model that converts sentences into a single dense vector. These vectors can be used in a popular machine learning model like logistic regression. The code below sets up the data for the doc2vec algorithm to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data to be Embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:41:38.394643Z",
     "start_time": "2018-08-09T17:41:38.352408Z"
    }
   },
   "outputs": [],
   "source": [
    "train_candidate_ids = train_sentences_df.candidate_id.astype(int).tolist()\n",
    "#dev_candidate_ids = (\n",
    "#    dev_sentences_df[dev_sentences_df.curated_dsh.notnull()]\n",
    "#    .candidate_id\n",
    "#    .astype(int)\n",
    "#    .tolist()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T17:41:46.914742Z",
     "start_time": "2018-08-09T17:41:38.396477Z"
    }
   },
   "outputs": [],
   "source": [
    "train_cands = (\n",
    "    session\n",
    "    .query(Candidate)\n",
    "    .filter(Candidate.id.in_(train_candidate_ids))\n",
    "    .all() \n",
    ")\n",
    "\n",
    "#dev_cands = (\n",
    "#    session\n",
    "#    .query(Candidate)\n",
    "#    .filter(Candidate.id.in_(dev_candidate_ids))\n",
    "#    .all() \n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:01:22.292466Z",
     "start_time": "2018-08-09T17:42:13.147554Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/disease_gene/disease_associates_gene/doc2vec/sentences/train_data_250k.txt\", \"w\") as f:\n",
    "    for c in tqdm.tqdm(train_cands):\n",
    "        f.write(c.get_parent().text + \"\\n\")\n",
    "\n",
    "#with open(\"data/disease_gene/disease_associates_gene/doc2vec/sentences/dev_data_labeled.txt\", \"w\") as f:\n",
    "#    for c in tqdm.tqdm(dev_cands):\n",
    "#        f.write(c.get_parent().text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_excel(\"data/sentence-labels-dev.xlsx\")\n",
    "dev_df = dev_df[dev_df.curated_dsh.isnull()]\n",
    "dev_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = dev_df.candidate_id.astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/doc2vec/dev_data_non_labeled.txt\", \"w\") as f:\n",
    "    for c in tqdm.tqdm(session.query(Candidate).filter(Candidate.id.in_(target_ids)).all()):\n",
    "        f.write(c.get_parent().text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data to Train On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT * from candidate\n",
    "WHERE split = 0 and type='disease_gene'\n",
    "ORDER BY RANDOM()\n",
    "LIMIT 500000;\n",
    "'''\n",
    "target_cids = [x[0] for x in session.execute(sql)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "with open(\"data/doc2vec/train_data_500k.txt\", \"w\") as f:\n",
    "    while True:\n",
    "        cands = session.query(Candidate).filter(Candidate.id.in_(target_cids)).offset(offset).limit(50000).all()\n",
    "        \n",
    "        if len(cands) == 0:\n",
    "            break\n",
    "            \n",
    "        for c in tqdm.tqdm(cands):\n",
    "            f.write(c.get_parent().text + \"\\n\")\n",
    "        \n",
    "        offset += 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When runnning doc2vec, make sure the first cell has finished running before starting the second. Subprocess creates a new process so both programs would be running simultaneously. As a result everything will slow down significantly. The submodule might not be compiled. If that is the case run this command: \n",
    "```bash\n",
    "gcc doc2vecc.c -o doc2vecc -lm -pthread -O3 -march=native -funroll-loops\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:34:49.130912Z",
     "start_time": "2018-08-13T13:34:48.695097Z"
    }
   },
   "outputs": [],
   "source": [
    "command = [\n",
    "    '../iclr2017/doc2vecc',\n",
    "    '-train', 'data/doc2vec/training_data/train_data_500k.txt',\n",
    "    '-word', 'data/disease_gene/disease_associates_gene/doc2vec/word_vectors/dag_word_vectors_26lfs_250k_500k.txt',\n",
    "    '-output', 'data/disease_gene/disease_associates_gene/doc2vec/doc_vectors/dag_doc_vectors_26lfs_250k_500k.txt',\n",
    "    '-cbow', '1',\n",
    "    '-size', '500',\n",
    "    '-negative', '5',\n",
    "    '-hs', '0',\n",
    "    '-threads', '5',\n",
    "    '-binary', '0',\n",
    "    '-iter', '30',\n",
    "    '-test', 'data/disease_gene/disease_associates_gene/doc2vec/sentences/train_data_250k.txt',\n",
    "    '-read-vocab', 'data/doc2vec/vocab/train_data_vocab_500k.txt'\n",
    "]\n",
    "subprocess.Popen(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = [\n",
    "    '../iclr2017/doc2vecc',\n",
    "    '-train', 'data/doc2vec/training_data/train_data_full_pubmed.txt',\n",
    "    '-word', 'data/doc2vec/word_vectors/full_dev_word_vectors.txt',\n",
    "    '-output', 'data/doc2vec/doc_vectors/full_dev_doc_vectors.txt',\n",
    "    '-cbow', '1',\n",
    "    '-size', '500',\n",
    "    '-negative', '5',\n",
    "    '-hs', '0',\n",
    "    '-threads', '5',\n",
    "    '-binary', '0',\n",
    "    '-iter', '30',\n",
    "    '-test', 'data/doc2vec/full_dev_set.txt',\n",
    "    '-read-vocab', 'data/doc2vec/vocab/train_data_vocab_full_pubmed.txt'\n",
    "]\n",
    "subprocess.Popen(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sparse Logistic Regression Disc Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train an SLR. To find the optimal hyperparameter settings this code uses a [random search](http://scikit-learn.org/stable/modules/grid_search.html) instead of iterating over all possible combinations of parameters. After the final model has been found, it is saved in the checkpoints folder to be loaded in the [next notebook](5.data-analysis.ipynb). Furthermore, the weights for the final model are output into a text file to be analyzed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:28:25.454689Z",
     "start_time": "2018-08-13T15:28:07.059211Z"
    }
   },
   "outputs": [],
   "source": [
    "doc2vec_X = pd.read_table(\"data/disease_gene/disease_associates_gene/doc2vec/doc_vectors/dag_doc_vectors_26lfs_250k_50k.txt\", \n",
    "                          compression=None, header=None, sep=\" \")\n",
    "doc2vec_X = doc2vec_X.values[:-1, :-1]\n",
    "doc2vec_dev_X = pd.read_table(\"data/doc2vec/doc_vectors/dev_doc_vectors_50k.txt.xz\",\n",
    "                              compression='xz', header=None, sep=\" \")\n",
    "doc2vec_dev_X = doc2vec_dev_X.values[:-1, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:28:43.289899Z",
     "start_time": "2018-08-13T15:28:25.456236Z"
    }
   },
   "outputs": [],
   "source": [
    "doc2vec_X_500k = pd.read_table(\"data/disease_gene/disease_associates_gene/doc2vec/doc_vectors/dag_doc_vectors_26lfs_250k_500k.txt\", \n",
    "                          compression=None, header=None, sep=\" \")\n",
    "doc2vec_X_500k = doc2vec_X_500k.values[:-1, :-1]\n",
    "doc2vec_dev_X_500k = pd.read_table(\"data/doc2vec/doc_vectors/dev_doc_vectors_500k.txt.xz\",\n",
    "                              compression='xz', header=None, sep=\" \")\n",
    "doc2vec_dev_X_500k = doc2vec_dev_X_500k.values[:-1, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:29:01.153074Z",
     "start_time": "2018-08-13T15:28:43.291331Z"
    }
   },
   "outputs": [],
   "source": [
    "doc2vec_X_all = pd.read_table(\"data/disease_gene/disease_associates_gene/doc2vec/doc_vectors/dag_doc_vectors_26lfs_250k_all.txt\",#\"data/doc2vec/doc_vectors/train_doc_vectors_full_pubmed.txt.xz\",\n",
    "                             compression=None, header=None, sep=\" \")\n",
    "doc2vec_X_all= doc2vec_X_all.values[:-1,:-1]\n",
    "doc2vec_dev_X_all = pd.read_table(\"data/doc2vec/doc_vectors/dev_doc_vectors_full_pubmed.txt.xz\",\n",
    "                              compression='xz', header=None, sep=\" \")\n",
    "doc2vec_dev_X_all = doc2vec_dev_X_all.values[:-1, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:29:07.230193Z",
     "start_time": "2018-08-13T15:29:01.154604Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(\n",
    "    train_sentences_df.sentence.values\n",
    ")\n",
    "dev_X = vectorizer.transform(dev_sentences_df.sentence.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:29:07.545747Z",
     "start_time": "2018-08-13T15:29:07.231948Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    X,\n",
    "    doc2vec_X,\n",
    "    doc2vec_X_500k,\n",
    "    doc2vec_X_all\n",
    "]\n",
    "\n",
    "dev_data = [\n",
    "    dev_X, \n",
    "    doc2vec_dev_X,\n",
    "    doc2vec_dev_X_500k,\n",
    "    doc2vec_dev_X_all\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    train_sentences_df.label.apply(lambda x: 1 if x > 0.5 else 0)\n",
    "    for i in range(len(data))\n",
    "]\n",
    "\n",
    "model_labels = [\n",
    "    \"Bag-Of-Words\",\n",
    "    \"Doc2Vec 50k\",\n",
    "    \"Doc2Vec 500k\",\n",
    "    \"Doc2Vec All\"\n",
    "]\n",
    "    \n",
    "lr_grids = [\n",
    "    {'C':np.linspace(1,100, num=4)}\n",
    "    for i in range(len(data))\n",
    "]\n",
    "\n",
    "final_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T15:29:07.575434Z",
     "start_time": "2018-08-13T15:29:07.549412Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T19:13:21.033145Z",
     "start_time": "2018-08-13T15:29:07.577197Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for train_data, grid, y_labels in zip(data, lr_grids, labels):\n",
    "    fit_model = GridSearchCV(lr_model, \n",
    "                         grid, cv=10, n_jobs=3, \n",
    "                         verbose=1, scoring='roc_auc', return_train_score=True)\n",
    "    fit_model.fit(train_data, y_labels)\n",
    "    final_models.append(fit_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T19:13:21.290315Z",
     "start_time": "2018-08-13T19:13:21.039543Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for model, model_label in zip(final_models, model_labels):\n",
    "    lr_result = pd.DataFrame(model.cv_results_)\n",
    "    plt.plot(lr_result[\"param_C\"], lr_result[\"mean_test_score\"], label=model_label)\n",
    "plt.legend()\n",
    "plt.xlabel(\"C (regularization parameter)\")\n",
    "plt.ylabel(\"Mean Test Score\")\n",
    "plt.title(\"BOW Training CV (10-fold)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T19:18:57.419533Z",
     "start_time": "2018-08-13T19:18:57.402558Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_marginals = []\n",
    "for model, test_data in zip(final_models, dev_data):\n",
    "    lr_marginals.append(model.best_estimator_.predict_proba(test_data)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T15:22:48.093767Z",
     "start_time": "2018-08-12T15:22:48.042144Z"
    }
   },
   "outputs": [],
   "source": [
    "marginals_df = (\n",
    "    pd.DataFrame(lr_marginals)\n",
    "    .transpose()\n",
    "    .rename(index=str, columns={0:'Bag of Words', 1: 'Doc2Vec 50k', 2:'Doc2Vec 500k', 3:'Doc2Vec All'})\n",
    ")\n",
    "marginals_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T15:23:43.045649Z",
     "start_time": "2018-08-12T15:23:43.000306Z"
    }
   },
   "outputs": [],
   "source": [
    "marginals_df.to_csv('data/disease_gene/disease_associates_gene/disc_model_marginals_26lfs_250k.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T15:48:33.359658Z",
     "start_time": "2018-08-12T15:47:29.319175Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sentences_df = pd.read_excel(\"data/disease_gene/sentence-labels.xlsx\")\n",
    "train_sentences_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T15:35:17.904316Z",
     "start_time": "2018-08-12T15:35:16.403743Z"
    }
   },
   "outputs": [],
   "source": [
    "dev_sentences_df = pd.read_excel(\"data/sentence-labels-dev.xlsx\")\n",
    "dev_sentences_df = dev_sentences_df[dev_sentences_df.curated_dsh.notnull()]\n",
    "dev_sentences_df = dev_sentences_df.sort_values(\"candidate_id\")\n",
    "dev_sentences_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T15:35:18.114153Z",
     "start_time": "2018-08-12T15:35:17.906231Z"
    }
   },
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT candidate_id FROM gold_label\n",
    "INNER JOIN Candidate ON Candidate.id=gold_label.candidate_id\n",
    "WHERE Candidate.split=0;\n",
    "'''\n",
    "cids = session.query(Candidate.id).filter(Candidate.id.in_([x[0] for x in session.execute(sql)]))\n",
    "L_train_labeled_gold = load_gold_labels(session, annotator_name='danich1', cids_query=cids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T15:48:42.173621Z",
     "start_time": "2018-08-12T15:48:42.138250Z"
    }
   },
   "outputs": [],
   "source": [
    "train_candidate_ids = train_sentences_df.candidate_id.astype(int).tolist()\n",
    "dev_candidate_ids = (\n",
    "    dev_sentences_df[dev_sentences_df.curated_dsh.notnull()]\n",
    "    .candidate_id\n",
    "    .astype(int)\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T15:48:53.843402Z",
     "start_time": "2018-08-12T15:48:43.781715Z"
    }
   },
   "outputs": [],
   "source": [
    "train_cands = (\n",
    "    session\n",
    "    .query(Candidate)\n",
    "    .filter(Candidate.id.in_(train_candidate_ids))\n",
    "    .all() \n",
    ")\n",
    "\n",
    "train_label_cands = (\n",
    "    session\n",
    "    .query(Candidate)\n",
    "    .filter(Candidate.id.in_(cids))\n",
    "    .all()\n",
    ")\n",
    "\n",
    "dev_cands = (\n",
    "    session\n",
    "    .query(Candidate)\n",
    "    .filter(Candidate.id.in_(dev_candidate_ids))\n",
    "    .all() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T11:25:25.658429Z",
     "start_time": "2018-08-12T15:48:57.028518Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.pytorch import LSTM\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':            0.01,\n",
    "    'embedding_dim': 250,\n",
    "    'hidden_dim':    250,\n",
    "    'n_epochs':      20,\n",
    "    'dropout':       0.10,\n",
    "    'seed':          1701\n",
    "}\n",
    "\n",
    "lstm = LSTM(n_threads=None)\n",
    "lstm.train(train_cands, train_sentences_df.label.values, X_dev=train_label_cands, \n",
    "           Y_dev=L_train_labeled_gold, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:16:34.455957Z",
     "start_time": "2018-08-13T13:16:33.044591Z"
    }
   },
   "outputs": [],
   "source": [
    "marginals = lstm.marginals(dev_cands)\n",
    "marginals_df[\"LSTM\"] = marginals\n",
    "marginals_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T13:16:48.515225Z",
     "start_time": "2018-08-13T13:16:48.481399Z"
    }
   },
   "outputs": [],
   "source": [
    "marginals_df.to_csv('data/disease_gene/disease_associates_gene/disc_model_marginals_26lfs_250k.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_models[2], open(\"data/best_model.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Top 100 and Bottom 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=\"data/doc2vec/doc_vectors/dev_doc_vectors_non_labeled_500k.txt.xz\"\n",
    "doc2vec_dev_X_500k_unlabeled = pd.read_table(data_file, sep=\" \", header=None)\n",
    "doc2vec_dev_X_500k_unlabeled = (\n",
    "    doc2vec_dev_X_500k_unlabeled\n",
    "    .head(doc2vec_dev_X_500k_unlabeled.shape[0]-1)\n",
    "    .drop(doc2vec_dev_X_500k_unlabeled.shape[1]-1, axis='columns')\n",
    "    .values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/doc2vec/dev_data_non_labled.txt\", \"r\") as f:\n",
    "    data = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences_non_labeled_df = pd.read_excel(\"data/sentence-labels-dev.xlsx\")\n",
    "dev_sentences_non_labeled_df = dev_sentences_non_labeled_df.query(\"curated_dsh.isnull()\")\n",
    "dev_sentences_non_labeled_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df = pd.DataFrame(\n",
    "    list(zip(data, final_models[2].predict_proba(doc2vec_dev_X_500k_unlabeled)[:,1])),\n",
    "    columns=['sentence','marginals'])\n",
    "unlabeled_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df = unlabeled_df.merge(dev_sentences_non_labeled_df, on=\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('data/100_sentences_list_A.xlsx')\n",
    "(unlabeled_df\n",
    "     .sort_values(\"marginals\", ascending=False)\n",
    "     .head(100)[[\"disease\", \"gene\", \"sentence\"]]\n",
    "     .to_excel(writer, sheet_name='sentences', index=False)\n",
    ")\n",
    "if writer.engine == 'xlsxwriter':\n",
    "    for sheet in writer.sheets.values():\n",
    "        sheet.freeze_panes(1, 0)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('data/100_sentences_list_B.xlsx')\n",
    "(unlabeled_df\n",
    "     .sort_values(\"marginals\", ascending=True)\n",
    "     .head(100)[[\"disease\", \"gene\", \"sentence\"]]\n",
    "     .to_excel(writer, sheet_name='sentences', index=False)\n",
    ")\n",
    "if writer.engine == 'xlsxwriter':\n",
    "    for sheet in writer.sheets.values():\n",
    "        sheet.freeze_panes(1, 0)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Best Model to Classify All Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_table(\"data/doc2vec/doc_vectors/full_train_doc_vectors.txt\", sep=\" \", header=None)\n",
    "train_X = train_X.head(train_X.shape[0]-1).drop(train_X.shape[1]-1, axis='columns').values\n",
    "\n",
    "dev_X = pd.read_table(\"data/doc2vec/doc_vectors/full_dev_doc_vectors.txt\", sep=\" \", header=None)\n",
    "dev_X = dev_X.head(dev_X.shape[0]-1).drop(dev_X.shape[1]-1, axis='columns').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dg_map_df = pd.read_table(\"data/doc2vec/full_train_dg_map.txt\", names=[\"disease_id\", \"gene_id\"])\n",
    "train_dg_map_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dg_map_df = pd.read_table(\"data/doc2vec/full_dev_dg_map.txt\", names=[\"disease_id\", \"gene_id\"])\n",
    "dev_dg_map_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = pickle.load(open(\"data/best_model.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dg_map_df['marginals'] = best_model.predict_proba(train_X)[:,1]\n",
    "train_dg_map_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dg_map_df['marginals'] = best_model.predict_proba(dev_X)[:,1]\n",
    "dev_dg_map_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dg_map_df.to_csv(\"data/training_set_marginals.tsv\", sep=\"\\t\", index=False)\n",
    "dev_dg_map_df.to_csv(\"data/dev_set_marginals.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:snorkeling]",
   "language": "python",
   "name": "conda-env-snorkeling-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
