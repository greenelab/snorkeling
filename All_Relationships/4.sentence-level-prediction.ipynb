{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Discriminator for Candidate Classification on the Sentence Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to train ML algorithms: Long Short Term Memory Neural Net (LSTM) and SparseLogisticRegression (SLR) for candidate classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUST RUN AT THE START OF EVERYTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the database for data extraction and load the Candidate subclass for the algorithms below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "username = \"danich1\"\n",
    "password = \"snorkel\"\n",
    "dbname = \"pubmeddb\"\n",
    "\n",
    "#Path subject to change for different os\n",
    "database_str = \"postgresql+psycopg2://{}:{}@/{}?host=/var/run/postgresql\".format(username, password, dbname)\n",
    "os.environ['SNORKELDB'] = database_str\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator, LabelAnnotator, load_marginals\n",
    "from snorkel.learning import SparseLogisticRegression\n",
    "from snorkel.learning.disc_models.rnn import reRNN\n",
    "from snorkel.learning.utils import RandomSearch\n",
    "from snorkel.models import Candidate, FeatureKey, candidate_subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_type = \"dg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edge_type == \"dg\":\n",
    "    DiseaseGene = candidate_subclass('DiseaseGene', ['Disease', 'Gene'])\n",
    "elif edge_type == \"gg\":\n",
    "    GeneGene = candidate_subclass('GeneGene', ['Gene1', 'Gene2'])\n",
    "elif edge_type == \"cg\":\n",
    "    CompoundGene = candidate_subclass('CompoundGene', ['Compound', 'Gene'])\n",
    "elif edge_type == \"cd\":\n",
    "    CompoundDisease = candidate_subclass('CompoundDisease', ['Compound', 'Disease'])\n",
    "else:\n",
    "    print(\"Please pick a valid edge type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load preprocessed data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will automatically load our labels and features that were generated in the [previous notebook](2.data-labeler.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labeler = LabelAnnotator(lfs=[])\n",
    "\n",
    "L_train = labeler.load_matrix(session, split=0)\n",
    "L_dev = labeler.load_matrix(session, split=1)\n",
    "L_test = labeler.load_matrix(session, split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Total Data Shape:\"\n",
    "print L_train.shape\n",
    "print L_dev.shape\n",
    "print L_test.shape\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "featurizer = FeatureAnnotator()\n",
    "\n",
    "F_train = featurizer.load_matrix(session, split=0)\n",
    "F_dev = featurizer.load_matrix(session, split=1)\n",
    "F_test = featurizer.load_matrix(session, split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Total Data Shape:\"\n",
    "print F_train.shape\n",
    "print F_dev.shape\n",
    "print F_test.shape\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sparse Logistic Regression Disc Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train an SLR. To find the optimal hyperparameter settings this code uses a [random search](http://scikit-learn.org/stable/modules/grid_search.html) instead of iterating over all possible combinations of parameters. After the final model has been found, it is saved in the checkpoints folder to be loaded in the [next notebook](5.data-analysis.ipynb). Furthermore, the weights for the final model are output into a text file to be analyzed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_marginals = load_marginals(session, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching over learning rates\n",
    "\"\"\" \n",
    "old code\n",
    "param_ranges = {\n",
    "    'lr' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'l1_penalty' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'l2_penalty' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rate_parameters = [\n",
    "        RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10), \n",
    "        RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10), \n",
    "        RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)]\n",
    "\n",
    "searcher = RandomSearch(SparseLogisticRegression, rate_parameters, F_train,\n",
    "                        Y_train=train_marginals, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(100)\n",
    "disc_model, run_stats = searcher.fit(F_dev, L_dev, n_threads=4, n_epochs=50, rebalance=0.5, print_freq=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_marginals = disc_model.marginals(F_test)\n",
    "LR_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"stratified_data/lstm_disease_gene_holdout/LR_data/LR_test_marginals.csv\"\n",
    "pd.DataFrame(LR_marginals, columns=[\"LR_Marginals\"]).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a LSTM Disc Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code trains an LSTM. An LSTM is a special type of recurrent nerual network that retains a memory of past values over period of time. ([Further explaination here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)). The problem with the code below is that sqlalchemy runs into an out of memory error on my computer during the preprocessing step. As a consequence we have to resort loading this data onto University of Pennsylvania's Performance Computing Cluster. The data that gets preprocessed is exported to a text file and then get shipped towards the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'stratified_data/lstm_disease_gene_holdout/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_marginals = load_marginals(session, split=0)\n",
    "np.savetxt(\"{}/train_marginals\".format(directory), train_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "train_kwargs = {\n",
    "    'lr':         0.001,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   10,\n",
    "    'dropout':    0.5,\n",
    "    'print_freq': 1,\n",
    "    'max_sentence_length': 1000,\n",
    "}\n",
    "\"\"\"\n",
    "lstm = reRNN(seed=100, n_threads=4)\n",
    "#lstm.train(train_cands, train_marginals[0:10], X_dev=dev_cands, Y_dev=L_dev[0:10], **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the Training data to an External File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "field_names = [\"disease_id\", \"disease_char_start\", \"disease_char_end\", \"gene_id\", \"gene_char_start\", \"gene_char_end\", \"sentence\", \"pubmed\"]\n",
    "chunksize = 100000\n",
    "start = 0\n",
    "\n",
    "with open('{}/train_candidates_ends.csv'.format(directory), 'wb') as g:\n",
    "    with open(\"{}/train_candidates_offsets.csv\".format(directory), \"wb\") as f:\n",
    "        with open(\"{}/train_candidates_sentences.csv\".format(directory), \"wb\") as h:\n",
    "            output = csv.writer(f)\n",
    "            writer = csv.DictWriter(h, fieldnames=field_names)\n",
    "            writer.writeheader()\n",
    "\n",
    "            while True:\n",
    "                train_cands = (\n",
    "                        session\n",
    "                        .query(DiseaseGene)\n",
    "                        .filter(DiseaseGene.split == 0)\n",
    "                        .order_by(DiseaseGene.id)\n",
    "                        .limit(chunksize)\n",
    "                        .offset(start)\n",
    "                        .all()\n",
    "                )\n",
    "\n",
    "                if not train_cands:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                for c in tqdm.tqdm(train_cands):\n",
    "                    data, ends = lstm._preprocess_data([c], extend=True)\n",
    "                    output.writerow(data[0])\n",
    "                    g.write(\"{}\\n\".format(ends[0]))\n",
    "                    \n",
    "                    row = {\n",
    "                    \"disease_id\": c.Disease_cid,\"disease_name\":c[0].get_span(),\n",
    "                    \"disease_char_start\":c[0].char_start, \"disease_char_end\": c[0].char_end, \n",
    "                    \"gene_id\": c.Gene_cid, \"gene_name\":c[1].get_span(), \n",
    "                    \"gene_char_start\":c[1].char_start, \"gene_char_end\":c[1].char_end, \n",
    "                    \"sentence\": c.get_parent().text, \"pubmed\", c.get_parent().get_parent().name\n",
    "                    }\n",
    "                \n",
    "                    writer.writerow(row)\n",
    "\n",
    "                start += chunksize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the word dictionary to an External File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"{}/train_word_dict.csv\".format(directory), 'w') as f:\n",
    "    output = csv.DictWriter(f, fieldnames=[\"Key\", \"Value\"])\n",
    "    output.writeheader()\n",
    "    for key in tqdm.tqdm(lstm.word_dict.d):\n",
    "        output.writerow({'Key':key, 'Value': lstm.word_dict.d[key]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Development Candidates to an External File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cands = (\n",
    "        session\n",
    "        .query(DiseaseGene)\n",
    "        .filter(DiseaseGene.split == 1)\n",
    "        .order_by(DiseaseGene.id)\n",
    "        .all()\n",
    ")\n",
    "\n",
    "dev_cand_labels = pd.read_csv(\"stratified_data/dev_set.csv\")\n",
    "hetnet_set = set(map(tuple,dev_cand_labels[dev_cand_labels[\"hetnet\"] == 1][[\"disease_ontology\", \"gene_id\"]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "field_names = [\n",
    "    \"disease_id\", \"disease_char_start\", \n",
    "    \"disease_char_end\", \"gene_id\", \n",
    "    \"gene_char_start\", \"gene_char_end\", \n",
    "    \"sentence\", \"pubmed\"\n",
    "]\n",
    "\n",
    "with open('{}/dev_candidates_offset.csv'.format(directory), 'wb') as g:\n",
    "    with open('{}/dev_candidates_labels.csv'.format(directory), 'wb') as f:\n",
    "        with open('{}/dev_candidates_sentences.csv'.format(directory), 'wb') as h:\n",
    "            \n",
    "            output = csv.writer(g)\n",
    "            label_output = csv.writer(f)\n",
    "            writer = csv.DictWriter(h, fieldnames=field_names)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for c in tqdm.tqdm(dev_cands):\n",
    "                data, ends = lstm._preprocess_data([c])\n",
    "                output.writerow(data[0])\n",
    "                label_output.writerow([1 if (c.Disease_cid, int(c.Gene_cid)) in hetnet_set else -1])\n",
    "                \n",
    "                row = {\n",
    "                \"disease_id\": c.Disease_cid,\"disease_name\":c[0].get_span(),\n",
    "                \"disease_char_start\":c[0].char_start, \"disease_char_end\": c[0].char_end, \n",
    "                \"gene_id\": c.Gene_cid, \"gene_name\":c[1].get_span(), \n",
    "                \"gene_char_start\":c[1].char_start, \"gene_char_end\":c[1].char_end, \n",
    "                \"sentence\": c.get_parent().text, \"pubmed\", c.get_parent().get_parent().name\n",
    "                }\n",
    "                \n",
    "                writer.writerow(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Test Candidates to an External File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cands = (\n",
    "        session\n",
    "        .query(DiseaseGene)\n",
    "        .filter(DiseaseGene.split == 2)\n",
    "        .order_by(DiseaseGene.id)\n",
    "        .all()\n",
    ")\n",
    "\n",
    "dev_cand_labels = pd.read_csv(\"stratified_data/test_set.csv\")\n",
    "hetnet_set = set(map(tuple,dev_cand_labels[dev_cand_labels[\"hetnet\"] == 1][[\"disease_ontology\", \"gene_id\"]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "field_names = [\"disease_id\", \"disease_char_start\", \"disease_char_end\", \"gene_id\", \"gene_char_start\", \"gene_char_end\", \"sentence\", \"pubmed\"]\n",
    "with open('{}/test_candidates_offset.csv'.format(directory), 'wb') as g:\n",
    "    with open('{}/test_candidates_labels.csv'.format(directory), 'wb') as f:\n",
    "        with open('{}/test_candidates_sentences.csv'.format(directory), 'wb') as h:\n",
    "            \n",
    "            output = csv.writer(g)\n",
    "            label_output = csv.writer(f)\n",
    "            writer = csv.DictWriter(h, fieldnames=field_names)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for c in tqdm.tqdm(test_cands):\n",
    "                data, ends = lstm._preprocess_data([c])\n",
    "                output.writerow(data[0])\n",
    "                label_output.writerow([1 if (c.Disease_cid, int(c.Gene_cid)) in hetnet_set else -1])\n",
    "                \n",
    "                row = {\n",
    "               \"disease_id\": c.Disease_cid,\"disease_name\":c[0].get_span(),\n",
    "                \"disease_char_start\":c[0].char_start, \"disease_char_end\": c[0].char_end, \n",
    "                \"gene_id\": c.Gene_cid, \"gene_name\":c[1].get_span(), \n",
    "                \"gene_char_start\":c[1].char_start, \"gene_char_end\":c[1].char_end, \n",
    "                \"sentence\": c.get_parent().text, \"pubmed\", c.get_parent().get_parent().name\n",
    "                }\n",
    "                \n",
    "                writer.writerow(row) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
