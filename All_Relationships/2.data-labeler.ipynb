{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUST RUN AT THE START OF EVERYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import operator\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import and_\n",
    "import tqdm\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "username = \"danich1\"\n",
    "password = \"snorkel\"\n",
    "dbname = \"pubmeddb\"\n",
    "\n",
    "#Path subject to change for different os\n",
    "database_str = \"postgresql+psycopg2://{}:{}@/{}?host=/var/run/postgresql\".format(username, password, dbname)\n",
    "os.environ['SNORKELDB'] = database_str\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator, LabelAnnotator\n",
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from snorkel.models import Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edge_type = \"dg\"\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if edge_type == \"dg\":\n",
    "    DiseaseGene = candidate_subclass('DiseaseGene', ['Disease', 'Gene'])\n",
    "    edge = \"disease_gene\"\n",
    "elif edge_type == \"gg\":\n",
    "    GeneGene = candidate_subclass('GeneGene', ['Gene1', 'Gene2'])\n",
    "    edge = \"gene_gene\"\n",
    "elif edge_type == \"cg\":\n",
    "    CompoundGene = candidate_subclass('CompoundGene', ['Compound', 'Gene'])\n",
    "    edge = \"compound_gene\"\n",
    "elif edge_type == \"cd\":\n",
    "    CompoundDisease = candidate_subclass('CompoundDisease', ['Compound', 'Disease'])\n",
    "    edge = \"compound_disease\"\n",
    "else:\n",
    "    print(\"Please pick a valid edge type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN = 0\n",
    "DEV = 1\n",
    "TEST = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at potential Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to look at loaded candidates from a given set. The constants represent the index to retrieve the training set, development set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = session.query(DiseaseGene).filter(DiseaseGene.split==TRAIN).limit(100)\n",
    "sv = SentenceNgramViewer(candidates, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigger Words for Label Function Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section attempts to find \"trigger\" words that will help distinguish true candidate relations from the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_freq_map(freq_map, pos_tag, pos_array, words):\n",
    "    for i, sens_pos_tags in enumerate(pos_array):\n",
    "        if pos_tag in sens_pos_tags:\n",
    "            freq_map[words[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigger_verbs = defaultdict(int)\n",
    "gene_trigger_adj = defaultdict(int)\n",
    "disease_trigger_adj = defaultdict(int)\n",
    "\n",
    "candidates = session.query(DiseaseGene).filter(DiseaseGene.split==TRAIN)\n",
    "for c in tqdm.tqdm(candidates):\n",
    "    candidate_context = c.get_contexts()\n",
    "    disease_start, disease_end = candidate_context[0].get_word_start(), candidate_context[0].get_word_end() \n",
    "    gene_start, gene_end = candidate_context[1].get_word_start(), candidate_context[1].get_word_end()\n",
    "    sentence = c.get_parent()\n",
    "    \n",
    "    #Verb\n",
    "    if gene_start < disease_start:\n",
    "        update_freq_map(trigger_verbs, \"VB\", sentence.pos_tags[gene_start:disease_start], sentence.words[gene_start:disease_start])\n",
    "    else:\n",
    "        update_freq_map(trigger_verbs, \"VB\", sentence.pos_tags[disease_start:gene_start], sentence.words[disease_start:gene_start])\n",
    "    \n",
    "    #Adjectives\n",
    "    if gene_start > 3:\n",
    "        update_freq_map(gene_trigger_adj, \"JJ\", sentence.pos_tags[gene_start-3:gene_start], sentence.words[gene_start-3:gene_start])\n",
    "    \n",
    "    if gene_end+3  < len(sentence.text):\n",
    "        update_freq_map(gene_trigger_adj, \"JJ\", sentence.pos_tags[gene_start:gene_start+3], sentence.words[gene_start:gene_start+3])\n",
    "    \n",
    "    if disease_start > 3:\n",
    "        update_freq_map(disease_trigger_adj, \"JJ\", sentence.pos_tags[disease_start-3:disease_start], sentence.words[disease_start-3:disease_start])\n",
    "    \n",
    "    if disease_end+3  < len(sentence.text):\n",
    "        update_freq_map(disease_trigger_adj, \"JJ\", sentence.pos_tags[disease_start:disease_start+3], sentence.words[disease_start:disease_start+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_verbs = sorted(fixed_trigger_verbs.items(), key=operator.itemgetter(1))\n",
    "sorted_verbs.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_gene_adj = sorted(fixed_gene_trigger_adj.items(), key=operator.itemgetter(1))\n",
    "sorted_gene_adj.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_disease_adj = sorted(fixed_disease_trigger_adj.items(), key=operator.itemgetter(1))\n",
    "sorted_disease_adj.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(sorted_verbs, columns=[\"word\", \"freq\"]).to_csv(\"verb_freq.csv\", index=False)\n",
    "pd.DataFrame(sorted_gene_adj, columns=[\"word\", \"freq\"]).to_csv(\"gadj_freq.csv\", index=False)\n",
    "pd.DataFrame(sorted_disease_adj, columns=[\"word\", \"freq\"]).to_csv(\"dadj_freq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the word cloud for particular edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_df = pd.read_csv(\"dadj_freq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq = {word:value for word, value in word_df.to_dict('split')['data']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcloud_obj = WordCloud(background_color='black', colormap=\"autumn\")\n",
    "wordcloud_obj.generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.imshow(wordcloud_obj)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the fundamental part of the project. Below are the label functions that are used to give a candidate a label of 1,0 or -1 which corresponds to correct relation, not sure and incorrection relation. The goal here is to develop functions that can label as many candidates as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if edge_type == \"dg\":\n",
    "    from utils.disease_gene_lf import *\n",
    "elif edge_type == \"gg\":\n",
    "    from utils.gene_gene_lf import *\n",
    "elif edge_type == \"cg\":\n",
    "    from utils.compound_gene_lf import *\n",
    "elif edge_type == \"cd\":\n",
    "    from utils.compound_disease_lf import *\n",
    "else:\n",
    "    print(\"Please pick a valid edge type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Label Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LF_DEBUG(C):\n",
    "    print \"Left Tokens\"\n",
    "    print get_left_tokens(c,window=3)\n",
    "    print\n",
    "    print \"Right Tokens\"\n",
    "    print get_right_tokens(c)\n",
    "    print\n",
    "    print \"Between Tokens\"\n",
    "    print get_between_tokens(c)\n",
    "    print \n",
    "    print \"Tagged Text\"\n",
    "    print get_tagged_text(c)\n",
    "    print re.search(r'{{B}} .* is a .* {{A}}',get_tagged_text(c))\n",
    "    print\n",
    "    print \"Get between Text\"\n",
    "    print get_text_between(c)\n",
    "    print len(get_text_between(c))\n",
    "    print \n",
    "    print \"Parent Text\"\n",
    "    print c.get_parent()\n",
    "    print\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = get_lfs() if not debug else [LF_DEBUG]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out Label Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled = []\n",
    "candidates = session.query(DiseaseGene).filter(DiseaseGene.split == 0).limit(1).all()\n",
    "\n",
    "for c in candidates:\n",
    "    print c\n",
    "    print get_text_between(c)\n",
    "    print c[1].sentence.entity_cids[c[1].get_word_start()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label The Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code will run through the label functions and label each candidate in the training and development groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeler = LabelAnnotator(lfs=LFs)\n",
    "\n",
    "cids = session.query(Candidate.id).filter(Candidate.split==0)\n",
    "%time L_train = labeler.apply(split=0, cids_query=cids, parallelism=5)\n",
    "\n",
    "cids = session.query(Candidate.id).filter(Candidate.split==1)\n",
    "%time L_dev = labeler.apply_existing(split=1, cids_query=cids, parallelism=5, clear=False)\n",
    "\n",
    "cids = session.query(Candidate.id).filter(Candidate.split==2)\n",
    "%time L_test = labeler.apply_existing(split=2, cids_query=cids, parallelism=5, clear=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Candidate Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code will generate features that some ml algorithms will use for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "featurizer = FeatureAnnotator()\n",
    "featurizer.apply(split=0, clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time F_dev = featurizer.apply_existing(split=1, parallelism=5, clear=False)\n",
    "%time F_test = featurizer.apply_existing(split=2, parallelism=5, clear=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work Around for above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below is a work around for the forever taking featurizer. Need to debug featurizer or at least check if there are snorkel updates on it on github, but anyway below code will write the feature rows to a text file. From that file psql will copy all the data to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "feature_key_hash = {}\n",
    "with open('feature_key_fixed.sql', 'rb') as d:\n",
    "    d.readline()\n",
    "    feature_key_reader = csv.reader(d, delimiter='\\t', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for row in tqdm.tqdm(feature_key_reader):\n",
    "        if len(row) <3:\n",
    "            print row\n",
    "        else:\n",
    "            feature_key_hash[row[1]] = row[2]\n",
    "            feat_counter = row[2]\n",
    "print feat_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.features import get_span_feats\n",
    "group = 0\n",
    "seen = set()\n",
    "with open('feature_key_fixed.sql', 'ab') as f:\n",
    "    with open('feature.sql', 'ab') as g:\n",
    "        #f.write(\"COPY feature_key(\\\"group\\\", name, id) from stdin with CSV DELIMITER '\t' QUOTE '\\\"';\\n\")\n",
    "        #g.write(\"COPY feature(value, candidate_id, key_id) from stdin with CSV DELIMITER '\t' QUOTE '\\\"';\\n\")\n",
    "        \n",
    "        feature_key_writer = csv.writer(f, delimiter='\\t',  quoting=csv.QUOTE_NONNUMERIC)\n",
    "        feature_writer = csv.writer(g, delimiter='\\t', quoting=csv.QUOTE_NONNUMERIC)\n",
    "        for c in tqdm.tqdm(session.query(Candidate).filter(Candidate.split==0).offset(2508430).all()):\n",
    "            for name, value in get_span_feats(c):\n",
    "                if name not in feature_key_hash:\n",
    "                    feature_key_hash[name] = feat_counter\n",
    "                    feat_counter = feat_counter + 1\n",
    "                    feature_key_writer.writerow([group, name, feature_key_hash[name]])\n",
    "                    \n",
    "                if (c.id, name) not in seen:\n",
    "                    feature_writer.writerow([value, c.id, feature_key_hash[name]])\n",
    "                    seen.add((c.id, name))\n",
    "            seen = set()\n",
    "        for c in tqdm.tqdm(session.query(Candidate).filter(Candidate.split==1).all()):\n",
    "            for name, value in get_span_feats(c):\n",
    "                if name in feature_key_hash:\n",
    "                    if (c.id, name) not in seen:\n",
    "                        feature_writer.writerow([value, c.id, feature_key_hash[name]])\n",
    "                        seen.add((c.id, name))\n",
    "       \n",
    "            seen = set()\n",
    "        for c in tqdm.tqdm(session.query(Candidate).filter(Candidate.split==2).all()):\n",
    "            for name, value in get_span_feats(c):\n",
    "                if name in feature_key_hash:\n",
    "                    if (c.id, name) not in seen:\n",
    "                        feature_writer.writerow([value, c.id, feature_key_hash[name]])\n",
    "                        seen.add((c.id, name))\n",
    "            seen = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import re\n",
    "seen = set()\n",
    "with open('feature_key_fixed.sql', 'rb') as f:\n",
    "    with open('feature_key.sql', 'wb') as g:\n",
    "        g.write(f.readline())\n",
    "        for line in tqdm.tqdm(f):\n",
    "            data = line.split(\"\\t\")\n",
    "            data[2] = re.sub(r'\\.\\d+','',data[2])\n",
    "            md5hash = hashlib.md5(data[2]).hexdigest()\n",
    "            if md5hash not in seen:\n",
    "                seen.add(md5hash)\n",
    "                g.write(\"\\t\".join(data))\n",
    "            else:\n",
    "                ids = re.search(r'\\d+', data[2]).group(0)\n",
    "                ids = int(ids) + 1\n",
    "                data[2] = \"{}\\r\\n\".format(ids)\n",
    "                seen.add(hashlib.md5(data[2]).hexdigest())\n",
    "                g.write(\"\\t\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "246125907it [07:05, 577922.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "seen = set()\n",
    "with open('feature.sql', 'rb') as f:\n",
    "    with open('feature_2.sql', 'wb') as g:\n",
    "        g.write(f.readline())\n",
    "        for line in tqdm.tqdm(f):\n",
    "            data = line.split(\"\\t\")\n",
    "            if  len(data) > 3:\n",
    "                data = [data[1], data[2], data[3]]\n",
    "            md5hash = hashlib.md5(line).hexdigest()\n",
    "            if md5hash not in seen:\n",
    "                data[2] = re.sub(r'\\.\\d+','',data[2])\n",
    "                seen.add(md5hash)\n",
    "                g.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Coverage Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before throwing our labels at a machine learning algorithm take a look at some quick stats. The code below will show the coverage of each label function and some other stat things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print L_train.lf_stats(session, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print L_train.get_candidate(session,21)\n",
    "print L_train.get_candidate(session,21).get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print L_train.shape\n",
    "print L_train[L_train < 0].shape\n",
    "print L_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print L_dev.lf_stats(session, )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
