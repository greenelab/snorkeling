{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUST RUN AT THE START OF EVERYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#Imports\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "username = \"danich1\"\n",
    "password = \"snorkel\"\n",
    "dbname = \"pubmeddb\"\n",
    "\n",
    "#Path subject to change for different os\n",
    "database_str = \"postgresql+psycopg2://{}:{}@/{}?host=/var/run/postgresql\".format(username, password, dbname)\n",
    "os.environ['SNORKELDB'] = database_str\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import PretaggedCandidateExtractor\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from utils.bigdata_utils import XMLMultiDocPreprocessor\n",
    "from utils.bigdata_utils import Tagger\n",
    "from sqlalchemy import func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the Pubmed Abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is designed to read and parse data gathered from pubtator. Pubtator outputs their annotated text in xml format, so that is the standard file format we are going to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " %time filter_df = pd.read_table('https://github.com/greenelab/pubtator/raw/631e86002e11c41cfcfb0043e60b84ab321bdae3/data/pubtator-hetnet-tags.tsv.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time grouped = filter_df.groupby('pubmed_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please change to your local document here\n",
    "working_path = '/home/danich1/Documents/Database/pubmed_docs.xml'\n",
    "xml_parser = XMLMultiDocPreprocessor(\n",
    "    path= working_path,\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()', tag_filter=set(filter_df['pubmed_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dg_tagger = Tagger(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus_parser = CorpusParser(fn=dg_tagger.tag)\n",
    "document_chunk = []\n",
    "\n",
    "for document in tqdm.tqdm(xml_parser.generate()):\n",
    "    \n",
    "    document_chunk.append(document)\n",
    "\n",
    "    # chunk the data because snorkel cannot \n",
    "    # scale properly yet\n",
    "    if len(document_chunk) >= 5e4:\n",
    "        corpus_parser.apply(document_chunk, parallelism=5, clear=False)\n",
    "        document_chunk = []\n",
    "    \n",
    "# If generator exhausts and there are still\n",
    "# document to parse\n",
    "if len(document_chunk) > 0:\n",
    "    corpus_parser.apply(data, parallelism=5, clear=False)\n",
    "    document_chunk = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get each candidate relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code below is designed to gather and tag each sentence found. **Note**: This does include the title of each abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk_size = 2e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_cand_to_db(extractor, sentences):\n",
    "    for split, sens in enumerate(sentences):\n",
    "        extractor.apply(sens, split=split, parallelism=5, clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_candidates(context_class, edge):\n",
    "    for i, label in enumerate([\"Train\", \"Dev\", \"Test\"]):\n",
    "        cand_len = session.query(context_class).filter(context_class.split == i).count()\n",
    "        print(\"Number of Candidates for {} edge and {} set: {}\".format(edge, label, cand_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This specifies the type of candidates to extract\n",
    "DiseaseGene = candidate_subclass('DiseaseGene', ['Disease', 'Gene'])\n",
    "dge = PretaggedCandidateExtractor(DiseaseGene, ['Disease', 'Gene'])\n",
    "\n",
    "GeneGene = candidate_subclass('GeneGene', ['Gene1', 'Gene2'])\n",
    "gge = PretaggedCandidateExtractor(GeneGene, ['Gene', 'Gene'])\n",
    "\n",
    "CompoundGene = candidate_subclass('CompoundGene', ['Compound', 'Gene'])\n",
    "cge = PretaggedCandidateExtractor(CompoundGene, ['Compound', 'Gene'])\n",
    "\n",
    "CompoundDisease = candidate_subclass('CompoundDisease', ['Compound','Disease'])\n",
    "cde = PretaggedCandidateExtractor(CompoundDisease, ['Compound', 'Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the seed for reproduction\n",
    "np.random.seed(100)\n",
    "total_sentences = 61615305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_list = np.random.choice([0,1,2], total_sentences, p=[0.7,0.2,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide the sentences into train, dev and test sets\n",
    "   \n",
    "#Grab the sentences!!!\n",
    "train_sens = set()\n",
    "dev_sens = set()\n",
    "test_sens = set()\n",
    "\n",
    "offset = 0\n",
    "category_index = 0\n",
    "sql_query = session.query(Document).limit(chunk_size)\n",
    "\n",
    "#divde and insert into the database\n",
    "while True:\n",
    "    documents = list(sql_query.offset(offset).all())\n",
    "    \n",
    "    if not documents:\n",
    "        break\n",
    "        \n",
    "    for doc in tqdm.tqdm(documents): \n",
    "        for s in doc.sentences:\n",
    "            \n",
    "            # Stratify the data into train, dev, test \n",
    "            category = category_list[category_index]\n",
    "            set_index = set_index + 1\n",
    "            \n",
    "            if category == 0:\n",
    "                train_sens.add(s)\n",
    "            elif category == 1:\n",
    "                dev_sens.add(s)\n",
    "            else:\n",
    "                test_sens.add(s)\n",
    "\n",
    "    # insert all the edge types\n",
    "    for edges in [dge, gge, cge, cde]:\n",
    "        insert_cand_to_db(edges, [train_sens, dev_sens, test_sens])\n",
    "        \n",
    "    offset = offset + chunk_size\n",
    "\n",
    "    #Reset for each chunk\n",
    "    train_sens = set()\n",
    "    dev_sens = set()\n",
    "    test_sens = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_candidates(DiseaseGene, 'DiseaseGene')\n",
    "print_candidates(GeneGene, 'GeneGene')\n",
    "print_candidates(CompoundGene, 'CompoundGene')\n",
    "print_candidates(CompoundDisease, 'CompoundDisease')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the Potential Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one cool thing about jupyter is that you can use this tool to look at candidates. Check it out after everything above has finished running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SET = 0\n",
    "DEVELOPMENT_SET = 1\n",
    "TEST_SET = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = session.query(DiseaseGene).filter(DiseaseGene.split==TEST_SET)\n",
    "sv = SentenceNgramViewer(candidates, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
