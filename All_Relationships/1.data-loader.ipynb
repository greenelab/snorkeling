{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUST RUN AT THE START OF EVERYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#Imports\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "username = \"danich1\"\n",
    "password = \"snorkel\"\n",
    "dbname = \"pubmeddb\"\n",
    "\n",
    "#Path subject to change for different os\n",
    "database_str = \"postgresql+psycopg2://{}:{}@/{}?host=/var/run/postgresql\".format(username, password, dbname)\n",
    "os.environ['SNORKELDB'] = database_str\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import PretaggedCandidateExtractor\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from utils.bigdata_utils import XMLMultiDocPreprocessor\n",
    "from utils.bigdata_utils import Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the Pubmed Abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is designed to read and parse data gathered from pubtator. Pubtator outputs their annotated text in xml format, so that is the standard file format we are going to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time filter_df = pd.read_table('/home/danich1/Documents/pubtator/data/pubtator-hetnet-tags.tsv.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time grouped = filter_df.groupby('pubmed_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please change to your local document here\n",
    "working_path = '/home/danich1/Documents/Database/pubmed_docs.xml'\n",
    "xml_parser = XMLMultiDocPreprocessor(\n",
    "    path= working_path,\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dg_tagger = Tagger(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus_parser = CorpusParser(fn=dg_tagger.tag)\n",
    "document_chunk = []\n",
    "\n",
    "for document in tqdm.tqdm(xml_parser.generate()):\n",
    "    \n",
    "    document_chunk.append(document)\n",
    "\n",
    "    # chunk the data because snorkel cannot \n",
    "    # scale properly yet\n",
    "    if len(document_chunk) >= 5e4:\n",
    "        corpus_parser.apply(document_chunk, parallelism=5, clear=False)\n",
    "        document_chunk = []\n",
    "\n",
    "# If generator exhausts and there are still\n",
    "# document to parse\n",
    "if len(document_chunk) > 0:\n",
    "    corpus_parser.apply(data, parallelism=5, clear=False)\n",
    "    document_chunk = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Documents: {}\".format(session.query(Document).count()))\n",
    "print(\"Sentences: {}\".format(session.query(Sentence).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get each candidate relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code below is designed to gather and tag each sentence found. **Note**: This does include the title of each abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk_size = 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This specifies that I want candidates that have a disease and gene mentioned in a given sentence\n",
    "DiseaseGene = candidate_subclass('DiseaseGene', ['Disease', 'Gene'])\n",
    "ce = PretaggedCandidateExtractor(DiseaseGene, ['Disease', 'Gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide the sentences into train, dev and test sets\n",
    "\n",
    "# set the seed for reproduction\n",
    "np.random.seed(100)\n",
    "   \n",
    "#Grab the sentences!!!\n",
    "train_sens = set()\n",
    "dev_sens = set()\n",
    "test_sens = set()\n",
    "\n",
    "offset = 0\n",
    "has_docs = True\n",
    "#divde and insert into the database\n",
    "while has_docs:\n",
    "    has_docs = False\n",
    "    for doc in tqdm.tqdm(session.query(Document).limit(chunk_size).offset(offset).all()): \n",
    "        has_docs = True\n",
    "        for s in doc.sentences:\n",
    "            \n",
    "            # Stratify the data into train, dev, test \n",
    "            category = np.random.choice([0,1,2], 1, p=[0.7,0.2,0.1])\n",
    "            \n",
    "            if category == 0:\n",
    "                train_sens.add(s)\n",
    "            elif category == 1:\n",
    "                dev_sens.add(s)\n",
    "            else:\n",
    "                test_sens.add(s)\n",
    "    if has_docs:\n",
    "        ce.apply(train_sens, split=0, parallelism=5, clear=False)\n",
    "        ce.apply(dev_sens, split=1, parallelism=5, clear=False)\n",
    "        ce.apply(test_sens, split=2, parallelism=5, clear=False)\n",
    "        offset = offset + chunk_size\n",
    "\n",
    "        #Reset for each chunk\n",
    "        train_sens = set()\n",
    "        dev_sens = set()\n",
    "        test_sens = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of Candidates: {}\".format(session.query(DiseaseGene).filter(DiseaseGene.split == 0).count()))\n",
    "print(\"Number of Candidates: {}\".format(session.query(DiseaseGene).filter(DiseaseGene.split == 1).count()))\n",
    "print(\"Number of Candidates: {}\".format(session.query(DiseaseGene).filter(DiseaseGene.split == 2).count()))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the Potential Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one cool thing about jupyter is that you can use this tool to look at candidates. Check it out after everything above has finished running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SET = 0\n",
    "DEVELOPMENT_SET = 1\n",
    "TEST_SET = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = session.query(DiseaseGene).filter(DiseaseGene.split==TRAINING_SET)\n",
    "sv = SentenceNgramViewer(candidates, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
