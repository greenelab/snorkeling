{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Organize the Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [previous notebook](1.data-loader.ipynb) we aim to stratify the candidates into the appropiate categories (training, development, test). Since the hard work (data insertion) was already done, this part is easy as it breaks down into relabeling the split column inside the Candidate table. The split column will be used throughout the rest of this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#Imports\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "username = \"danich1\"\n",
    "password = \"snorkel\"\n",
    "dbname = \"pubmeddb\"\n",
    "\n",
    "#Path subject to change for different os\n",
    "database_str = \"postgresql+psycopg2://{}:{}@/{}?host=/var/run/postgresql\".format(username, password, dbname)\n",
    "os.environ['SNORKELDB'] = database_str\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import  candidate_subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This specifies the type of candidates to extract\n",
    "DiseaseGene = candidate_subclass('DiseaseGene', ['Disease', 'Gene'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Stratified File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disease_ontology_df = pd.read_csv('https://raw.githubusercontent.com/dhimmel/disease-ontology/052ffcc960f5897a0575f5feff904ca84b7d2c1d/data/xrefs-prop-slim.tsv', sep=\"\\t\")\n",
    "disease_ontology_df = disease_ontology_df.drop_duplicates([\"doid_code\", \"doid_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gene_entrez_df = pd.read_csv('https://raw.githubusercontent.com/dhimmel/entrez-gene/a7362748a34211e5df6f2d185bb3246279760546/data/genes-human.tsv', sep=\"\\t\")\n",
    "gene_entrez_df = gene_entrez_df[[\"GeneID\", \"Symbol\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Each Disease to Each Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gene_entrez_df['dummy_key'] =0\n",
    "disease_ontology_df['dummy_key'] = 0\n",
    "dg_map_df = gene_entrez_df.merge(disease_ontology_df[[\"doid_code\", \"doid_name\", \"dummy_key\"]], on='dummy_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label All Pairs Whether or Not They are in Hetnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hetnet_kb_df = pd.read_csv(\"hetnet_dg_kb.csv\")\n",
    "hetnet_set = set(map(lambda x: tuple(x), hetnet_kb_df.values))\n",
    "hetnet_labels = np.ones(dg_map_df.shape[0]) * -1\n",
    "\n",
    "for index, row in tqdm.tqdm(dg_map_df.iterrows()):\n",
    "    if (row[\"doid_code\"], row[\"GeneID\"]) in hetnet_set:\n",
    "        hetnet_labels[index] = 1 \n",
    "    \n",
    "dg_map_df[\"hetnet\"] = hetnet_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See if D-G Pair is in Pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pubmed_dg_pairs = set({})\n",
    "cands = []\n",
    "chunk_size = 1e5\n",
    "offset = 0\n",
    "\n",
    "while True:\n",
    "    cands = session.query(DiseaseGene).limit(chunk_size).offset(offset).all()\n",
    "    \n",
    "    if not cands:\n",
    "        break\n",
    "        \n",
    "    for candidate in tqdm.tqdm(cands):\n",
    "        pubmed_dg_pairs.add((candidate.Disease_cid, candidate.Gene_cid))\n",
    "    \n",
    "    offset = offset + chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pubmed_labels = np.ones(dg_map_df.shape[0]) * -1\n",
    "\n",
    "for index, row in tqdm.tqdm(dg_map_df.iterrows()):\n",
    "    if (row[\"doid_code\"], str(row[\"GeneID\"])) in pubmed_dg_pairs:\n",
    "        pubmed_labels[index] = 1\n",
    "\n",
    "dg_map_df[\"pubmed\"] = pubmed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg_map_df = dg_map_df.rename(index=str, columns={\"GeneID\": \"gene_id\", \"doid_code\": \"disease_id\", \"doid_name\": \"disease_name\", \"Symbol\":\"gene_name\"})\n",
    "dg_map_df[\"hetnet\"] = dg_map_df[\"hetnet\"].astype(int)\n",
    "dg_map_df[\"pubmed\"] = dg_map_df[\"pubmed\"].astype(int)\n",
    "dg_map_df.to_csv(\"dg_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the Candidate split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below changes the split column of the candidate table as mentioned above. Using sqlalchemy and the chunking strategy, every candidate that has the particular disease entity id (DOID:3393) will be given the category of 2. 2 Representes the testing set which will be used in the rest of the notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_map_df = pd.read_csv(\"dg_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12623, 6)\n",
      "(150280, 6)\n",
      "\n",
      "(9478, 6)\n",
      "(3145, 6)\n",
      "(140802, 6)\n",
      "(7510447, 6)\n"
     ]
    }
   ],
   "source": [
    "print dg_map_df[(dg_map_df[\"hetnet\"] == 1)].shape\n",
    "print dg_map_df[(dg_map_df[\"pubmed\"]== 1)].shape\n",
    "print\n",
    "print dg_map_df[(dg_map_df[\"hetnet\"] == 1)&(dg_map_df[\"pubmed\"]== 1)].shape\n",
    "print dg_map_df[(dg_map_df[\"hetnet\"] == 1)&(dg_map_df[\"pubmed\"]== -1)].shape\n",
    "print dg_map_df[(dg_map_df[\"hetnet\"] == -1)&(dg_map_df[\"pubmed\"]== 1)].shape\n",
    "print dg_map_df[(dg_map_df[\"hetnet\"] == -1)&(dg_map_df[\"pubmed\"]== -1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "dev_size = 0.2\n",
    "training_size = 0.7\n",
    "random_seed = 100\n",
    "\n",
    "sizes = []\n",
    "sizes.append(dg_map_df[(dg_map_df[\"hetnet\"] == 1)&(dg_map_df[\"pubmed\"]== 1)].shape[0])\n",
    "sizes.append(dg_map_df[(dg_map_df[\"hetnet\"] == 1)&(dg_map_df[\"pubmed\"]== -1)].shape[0])\n",
    "sizes.append(dg_map_df[(dg_map_df[\"hetnet\"] == -1)&(dg_map_df[\"pubmed\"]== 1)].shape[0])\n",
    "sizes.append(dg_map_df[(dg_map_df[\"hetnet\"] == -1)&(dg_map_df[\"pubmed\"]== -1)].shape[0])\n",
    "\n",
    "dummy_dg_map = dg_map_df\n",
    "\n",
    "for data_size, file_name in zip([test_size, dev_size], [\"stratified_data/test_set.csv\", \"stratified_data/dev_set.csv\"]):\n",
    "    adjusted_size = np.round(np.array(sizes) * data_size).astype(int)\n",
    "\n",
    "    hetnet_pubmed = dummy_dg_map[(dummy_dg_map[\"hetnet\"] == 1)&(dummy_dg_map[\"pubmed\"]== 1)].sample(adjusted_size[0], random_state=random_seed)\n",
    "    hetnet_no_pubmed = dummy_dg_map[(dummy_dg_map[\"hetnet\"] == 1)&(dummy_dg_map[\"pubmed\"]== -1)].sample(adjusted_size[1], random_state=random_seed)\n",
    "    no_hetnet_pubmed = dummy_dg_map[(dummy_dg_map[\"hetnet\"] == -1)&(dummy_dg_map[\"pubmed\"]== 1)].sample(adjusted_size[2], random_state=random_seed)\n",
    "    no_hetnet_no_pubmed = dummy_dg_map[(dummy_dg_map[\"hetnet\"] == -1)&(dummy_dg_map[\"pubmed\"]== -1)].sample(10000, random_state=random_seed)\n",
    "    \n",
    "    final_dataset = hetnet_pubmed.append(hetnet_no_pubmed).append(no_hetnet_pubmed).append(no_hetnet_no_pubmed)\n",
    "    final_dataset.to_csv(file_name, index=False)\n",
    "    dummy_dg_map = dummy_dg_map.drop(final_dataset.index)\n",
    "\n",
    "final_dataset = dummy_dg_map[(dummy_dg_map[\"hetnet\"] == 1)&(dummy_dg_map[\"pubmed\"]== 1)]\n",
    "final_dataset = final_dataset.append(dummy_dg_map[(dummy_dg_map[\"hetnet\"] == -1)&(dummy_dg_map[\"pubmed\"]== 1)])\n",
    "final_dataset = final_dataset.append(dummy_dg_map[(dummy_dg_map[\"hetnet\"] == 1)&(dummy_dg_map[\"pubmed\"]== -1)])\n",
    "final_dataset = final_dataset.append(dummy_dg_map[(dummy_dg_map[\"hetnet\"] == -1)&(dummy_dg_map[\"pubmed\"]== -1)].sample(10000, random_state=random_seed))\n",
    "final_dataset.to_csv(\"stratified_data/training_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-categorize The Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"stratified_data/test_set.csv\")\n",
    "test_set = set(map(tuple, test_df[(test_df[\"pubmed\"] == 1)][[\"disease_id\",\"gene_id\"]].values))\n",
    "\n",
    "dev_df = pd.read_csv(\"stratified_data/dev_set.csv\")\n",
    "dev_set = set(map(tuple, dev_df[(dev_df[\"pubmed\"] == 1)][[\"disease_id\",\"gene_id\"]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:03<00:00, 33313.21it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 39434.15it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 39713.41it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 39295.52it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 34003.53it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 33744.55it/s]\n",
      "100%|██████████| 100000/100000 [00:03<00:00, 31788.45it/s]\n",
      "100%|██████████| 100000/100000 [00:03<00:00, 31432.27it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 36406.16it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 34926.87it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 33609.97it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 33464.84it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 36913.60it/s]\n",
      "100%|██████████| 100000/100000 [00:03<00:00, 32885.78it/s]\n",
      "100%|██████████| 100000/100000 [00:03<00:00, 32596.73it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 37292.80it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 38911.20it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 39586.43it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 39728.98it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 36388.86it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 34095.84it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 39577.56it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 33489.34it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 39717.70it/s]\n",
      "100%|██████████| 100000/100000 [00:07<00:00, 12768.48it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 33701.51it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 39443.99it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 33517.92it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 33494.78it/s]\n",
      "100%|██████████| 100000/100000 [00:03<00:00, 33057.93it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 33577.61it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 34376.23it/s]\n",
      "100%|██████████| 100000/100000 [00:07<00:00, 12824.07it/s]\n",
      "100%|██████████| 100000/100000 [00:03<00:00, 32785.36it/s]\n",
      "100%|██████████| 100000/100000 [00:03<00:00, 33251.50it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 33840.27it/s]\n",
      "100%|██████████| 100000/100000 [00:08<00:00, 12364.09it/s]\n",
      "100%|██████████| 100000/100000 [00:09<00:00, 10788.85it/s]\n",
      "100%|██████████| 30137/30137 [00:02<00:00, 10529.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 18s, sys: 44.1 s, total: 11min 2s\n",
      "Wall time: 53min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cands = []\n",
    "chunk_size = 1e5\n",
    "offset = 0\n",
    "\n",
    "while True:\n",
    "    cands = session.query(DiseaseGene).limit(chunk_size).offset(offset).all()\n",
    "    \n",
    "    if not cands:\n",
    "        break\n",
    "        \n",
    "    for candidate in tqdm.tqdm(cands):\n",
    "        if (candidate.Disease_cid, int(candidate.Gene_cid)) in test_set:\n",
    "            candidate.split = 2\n",
    "        elif (candidate.Disease_cid, int(candidate.Gene_cid)) in dev_set:\n",
    "            candidate.split = 1\n",
    "        else:\n",
    "            candidate.split = 0\n",
    "        \n",
    "        session.add(candidate)\n",
    "    \n",
    "    offset = offset + chunk_size\n",
    "# persist the changes into the database\n",
    "session.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
