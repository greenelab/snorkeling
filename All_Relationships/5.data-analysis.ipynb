{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets See How The Disc Models Preformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to analyze the disc models performance and to answer the question does Long Short Term Memory Neural Net (LSTM) outperform SparseLogisticRegression (SLR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUST RUN AT THE START OF EVERYTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the database and other helpful functions for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, roc_curve, auc, f1_score\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "username = \"danich1\"\n",
    "password = \"snorkel\"\n",
    "dbname = \"pubmeddb\"\n",
    "\n",
    "#Path subject to change for different os\n",
    "database_str = \"postgresql+psycopg2://{}:{}@/{}?host=/var/run/postgresql\".format(username, password, dbname)\n",
    "os.environ['SNORKELDB'] = database_str\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator, LabelAnnotator, load_marginals\n",
    "from snorkel.learning import SparseLogisticRegression\n",
    "from snorkel.learning.disc_models.rnn import reRNN\n",
    "from snorkel.learning.utils import RandomSearch\n",
    "from snorkel.models import Candidate, FeatureKey, candidate_subclass\n",
    "from snorkel.utils import get_as_dict\n",
    "from tree_structs import corenlp_to_xmltree\n",
    "from treedlib import compile_relation_feature_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edge_type = \"dg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if edge_type == \"dg\":\n",
    "    DiseaseGene = candidate_subclass('DiseaseGene', ['Disease', 'Gene'])\n",
    "elif edge_type == \"gg\":\n",
    "    GeneGene = candidate_subclass('GeneGene', ['Gene1', 'Gene2'])\n",
    "elif edge_type == \"cg\":\n",
    "    CompoundGene = candidate_subclass('CompoundGene', ['Compound', 'Gene'])\n",
    "elif edge_type == \"cd\":\n",
    "    CompoundDisease = candidate_subclass('CompoundDisease', ['Compound', 'Disease'])\n",
    "else:\n",
    "    print(\"Please pick a valid edge type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we load the test dataset in conjunction with the previously trained disc models. Each algorithm will output a probability of a candidate being a true candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurizer = FeatureAnnotator()\n",
    "labeler = LabelAnnotator(lfs=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "L_test = labeler.load_matrix(session,split=2)\n",
    "F_test = featurizer.load_matrix(session, split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model = SparseLogisticRegression()\n",
    "lstm_model = reRNN(seed=100, n_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model.load(save_dir='checkpoints/grid_search/', model_name=\"SparseLogisticRegression_1\")\n",
    "lstm_model.load(save_dir='checkpoints/rnn', model_name=\"RNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the Data for analysis below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the necessary data such as top predicted candidates and the traning marginals from the SparseLogisticRegression (SLR) model. LSTM and deep learning data to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_marginals = lr_model.marginals(F_test)\n",
    "rnn_marginals = lstm.marginals(F_test)\n",
    "marginal_df = pd.DataFrame([lr_marginals, rnn_marginals], columns=[\"LR_Marginals\", \"RNN_marginals\"])\n",
    "marginal_df.to_csv(\"disc_marginals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_marginals = pd.read_csv(\"disc_marginals.csv\")\n",
    "top_pos_predict_model_marginals = model_marginals.sort_values(\"LR_Marginals\", ascending=False).head(10)\n",
    "top_neg_predict_model_marginals = model_marginals.sort_values(\"LR_Marginals\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "pos_feature_freq = defaultdict(int)\n",
    "for index in tqdm.tqdm(top_pos_predict_model_marginals.index):\n",
    "    top_match_feat = F_test[index,:].nonzero()[1]\n",
    "    for feature in lr_df[\"Feature\"][top_match_feat]:\n",
    "        pos_feature_freq[feature] += 1\n",
    "pos_features_df = pd.DataFrame(pos_feature_freq.items(), columns=[\"Feature\", \"Frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "neg_feature_freq = defaultdict(int)\n",
    "for index in tqdm.tqdm(top_neg_predict_model_marginals.index):\n",
    "    top_match_feat = F_test[index,:].nonzero()[1]\n",
    "    for feature in lr_df[\"Feature\"][top_match_feat]:\n",
    "        neg_feature_freq[feature] += 1\n",
    "neg_features_df = pd.DataFrame(neg_feature_freq.items(), columns=[\"Feature\", \"Frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_features_df.sort_values(\"Frequency\", ascending=False).to_csv('POS_LR_Feat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_features_df.sort_values(\"Frequency\", ascending=False).to_csv(\"NEG_LR_Feat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code shows the amount of true positives, false positives, true negatives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = lr_model.error_analysis(session, F_test, L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = lstm_model.error_analysis(session, F_test, L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the probabilities calculated above, we can create a [Receiver Operator Curve](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html) (ROC) graph to measure the false positive rate and the true positive rate at each calculated threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "\n",
    "for model_marginals, color in zip([\"LR_Marginals\", \"RNN_marginals\"], [\"darkorange\", \"red\"]):\n",
    "    fpr, tpr, _= roc_curve(L_test[0:].todense(), marginal_df[model_marginals])\n",
    "    model_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=color, label=\"{} curve (area = {0.2f})\".format(model_auc))\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Accuracy ROC')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision vs Recall Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code produces a [Precision-Recall](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) graph, which shows the trade off between [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) at each given probability threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model_marginals, color in zip([\"LR_Marginals\", \"RNN_marginals\"], [\"darkorange\", \"red\"]):\n",
    "    precision, recall, _=  precision_recall_curve(L_test[0:].todense(), marginal_df[model_marginals])\n",
    "    model_f1 = f1_score(L_test[0:].todense(), marginal_df[model_marginals])\n",
    "    plt.plot(fpr, tpr, color=color, label=\"{} curve (area = {0.2f})\".format(model_f1))\n",
    "\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Precision vs Recall')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Model Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Picture of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a deeper look into the SLR model, we can see that the highest weighted features are all cancer related. This leads one to think that majority of these abstracts are cancer related. Looking at the distribution of the weights it follows a normal distribution with a significant number of zeros. These zeros are desired which means majority of these feautres do not have significant weight to determine if a candidate is true or not. Furthermore, using the last cell of this block we can take a close look at the [dependency tree](https://nlp.stanford.edu/software/stanford-dependencies.shtml) stanford's core nlp used for generating features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_df = pd.read_csv(\"LR_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_df = lr_df.sort_values(\"Weight\", ascending=False, kind='mergesort')\n",
    "weight_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(weight_df[\"Weight\"])\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of LR Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cand = session.query(Candidate).filter(Candidate.id==674118).all()\n",
    "print cand\n",
    "print cand[0].get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cand = session.query(Candidate).filter(Candidate.id == 19841894).one()\n",
    "print cand\n",
    "xmltree = corenlp_to_xmltree(get_as_dict(cand.get_parent()))\n",
    "xmltree.render_tree(highlight=[range(cand[0].get_word_start(), cand[0].get_word_end() + 1), range(cand[1].get_word_start(), cand[1].get_word_end()+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Deeper Look into the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above cells we saw that the SLR model is somewhat behaving as we expected. This section here attempts to dive deeper into the model and examine the top predicted candidates (both positive and negative). After gathering each candidate, we take a consensus of all the features these top candidates share, so we can have a better understanding on how these predictions came to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_features_df = pd.read_csv(\"POS_LR_Feat.csv\")\n",
    "neg_features_df = pd.read_csv(\"NEG_LR_Feat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_features_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_features_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
