{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUST RUN AT THE START OF EVERYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "username = \"danich1\"\n",
    "password = \"snorkel\"\n",
    "dbname = \"pubmeddb\"\n",
    "\n",
    "#Path subject to change for different os\n",
    "database_str = \"postgresql+psycopg2://{}:{}@/{}?host=/var/run/postgresql\".format(username, password, dbname)\n",
    "os.environ['SNORKELDB'] = database_str\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator, LabelAnnotator, load_marginals\n",
    "from snorkel.learning import SparseLogisticRegression\n",
    "from snorkel.learning.disc_models.rnn import reRNN\n",
    "from snorkel.learning.utils import RandomSearch\n",
    "from snorkel.models import Candidate, FeatureKey, candidate_subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edge_type = \"dg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if edge_type == \"dg\":\n",
    "    DiseaseGene = candidate_subclass('DiseaseGene', ['Disease', 'Gene'])\n",
    "elif edge_type == \"gg\":\n",
    "    GeneGene = candidate_subclass('GeneGene', ['Gene1', 'Gene2'])\n",
    "elif edge_type == \"cg\":\n",
    "    CompoundGene = candidate_subclass('CompoundGene', ['Compound', 'Gene'])\n",
    "elif edge_type == \"cd\":\n",
    "    CompoundDisease = candidate_subclass('CompoundDisease', ['Compound', 'Disease'])\n",
    "else:\n",
    "    print(\"Please pick a valid edge type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load preprocessed data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time, this code will automatically load our labels that were generated in the previous file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3830137it [00:06, 582573.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.82 s, sys: 1.23 s, total: 10 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labeler = LabelAnnotator(lfs=[])\n",
    "\n",
    "#L_train = labeler.load_matrix(session,split=0)\n",
    "L_dev = labeler.load_matrix(session,split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Total Data Shape:\"\n",
    "print L_train.shape\n",
    "print L_dev.shape\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "featurizer = FeatureAnnotator()\n",
    "\n",
    "F_train = featurizer.load_matrix(session, split=0)\n",
    "F_dev = featurizer.load_matrix(session, split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Total Data Shape:\"\n",
    "print F_train.shape\n",
    "print F_dev.shape\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Disc Model Classification of Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Logistic Regression Disc Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time train_marginals = load_marginals(session, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Searching over learning rate\n",
    "param_ranges = {\n",
    "    'lr' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'l1_penalty' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'l2_penalty' : [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "}\n",
    "model_hyperparams = {\n",
    "    'n_epochs' : 50,\n",
    "    'rebalance' : 0.5,\n",
    "    'print_freq' : 25\n",
    "}\n",
    "searcher = RandomSearch(SparseLogisticRegression, param_ranges, F_train,\n",
    "                        Y_train=train_marginals, n=5, model_hyperparams=model_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(100)\n",
    "disc_model, run_stats = searcher.fit(F_dev, L_dev, n_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, b = disc_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the weights and features for further processing\n",
    "annot_select_query = FeatureKey.__table__.select().order_by(FeatureKey.id)\n",
    "with open(\"LR_model.csv\", \"w\") as f:\n",
    "    fieldnames = [\"Weight\", \"Feature\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for weight, feature in tqdm.tqdm(zip(w, session.execute(annot_select_query))):\n",
    "        writer.writerow({\"Weight\": weight, \"Feature\":feature[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Recurrent Neural Net Disc Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 512 ms, total: 11.9 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%time train_marginals = load_marginals(session, split=0)\n",
    "np.savetxt(\"pmacs/train_marginals\", train_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 98 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_kwargs = {\n",
    "    'lr':         0.001,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   10,\n",
    "    'dropout':    0.5,\n",
    "    'print_freq': 1,\n",
    "    'max_sentence_length': 1000,\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=100, n_threads=4)\n",
    "#lstm.train(train_cands, train_marginals[0:10], X_dev=dev_cands, Y_dev=L_dev[0:10], **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [04:52<00:00, 342.14it/s]\n",
      "100%|██████████| 100000/100000 [04:48<00:00, 346.14it/s]\n",
      "100%|██████████| 100000/100000 [04:42<00:00, 354.45it/s]\n",
      "100%|██████████| 100000/100000 [04:36<00:00, 361.64it/s]\n",
      "100%|██████████| 100000/100000 [04:26<00:00, 374.95it/s]\n",
      "100%|██████████| 100000/100000 [04:21<00:00, 382.21it/s]\n",
      "100%|██████████| 100000/100000 [04:22<00:00, 380.31it/s]\n",
      "100%|██████████| 100000/100000 [04:29<00:00, 371.71it/s]\n",
      "100%|██████████| 100000/100000 [04:25<00:00, 377.00it/s]\n",
      "100%|██████████| 100000/100000 [04:24<00:00, 378.30it/s]\n",
      "100%|██████████| 100000/100000 [04:33<00:00, 365.49it/s]\n",
      "100%|██████████| 100000/100000 [04:26<00:00, 375.65it/s]\n",
      "100%|██████████| 100000/100000 [04:28<00:00, 373.04it/s]\n",
      "100%|██████████| 100000/100000 [04:23<00:00, 379.84it/s]\n",
      "100%|██████████| 100000/100000 [04:16<00:00, 389.49it/s]\n",
      "100%|██████████| 100000/100000 [04:13<00:00, 394.21it/s]\n",
      "100%|██████████| 100000/100000 [04:12<00:00, 395.55it/s]\n",
      "100%|██████████| 100000/100000 [04:10<00:00, 398.96it/s]\n",
      "100%|██████████| 100000/100000 [04:09<00:00, 400.19it/s]\n",
      "100%|██████████| 100000/100000 [04:09<00:00, 401.54it/s]\n",
      "100%|██████████| 100000/100000 [04:10<00:00, 399.45it/s]\n",
      "100%|██████████| 100000/100000 [04:08<00:00, 401.84it/s]\n",
      "100%|██████████| 100000/100000 [04:10<00:00, 400.00it/s]\n",
      "100%|██████████| 100000/100000 [04:10<00:00, 398.66it/s]\n",
      "100%|██████████| 100000/100000 [04:09<00:00, 400.11it/s]\n",
      "100%|██████████| 100000/100000 [04:06<00:00, 406.31it/s]\n",
      "100%|██████████| 83871/83871 [03:30<00:00, 398.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "chunksize = 100000\n",
    "start = 0\n",
    "with open('pmacs/train_candidates_ends.csv', 'wb') as g:\n",
    "    with open(\"pmacs/train_candidates_offsets.csv\", \"wb\") as f:\n",
    "        while True:\n",
    "            train_cands = session.query(DiseaseGene).filter(DiseaseGene.split == 0).order_by(DiseaseGene.id).limit(chunksize).offset(start).all()\n",
    "            \n",
    "            if not train_cands:\n",
    "                break\n",
    "                \n",
    "            output = csv.writer(f)\n",
    "            for c in tqdm.tqdm(train_cands):\n",
    "                data, ends = lstm._preprocess_data([c], extend=True)\n",
    "                output.writerow(data[0])\n",
    "                g.write(\"{}\\n\".format(ends[0]))\n",
    "            \n",
    "            start += chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/443430 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 45258/443430 [00:00<00:00, 452579.35it/s]\u001b[A\n",
      " 21%|██        | 91831/443430 [00:00<00:00, 456443.15it/s]\u001b[A\n",
      " 32%|███▏      | 140205/443430 [00:00<00:00, 464302.02it/s]\u001b[A\n",
      " 42%|████▏     | 188022/443430 [00:00<00:00, 467369.15it/s]\u001b[A\n",
      " 54%|█████▎    | 237375/443430 [00:00<00:00, 473739.61it/s]\u001b[A\n",
      " 64%|██████▍   | 285669/443430 [00:00<00:00, 476461.50it/s]\u001b[A\n",
      " 76%|███████▌  | 335339/443430 [00:00<00:00, 481497.66it/s]\u001b[A\n",
      " 87%|████████▋ | 384756/443430 [00:00<00:00, 485215.05it/s]\u001b[A\n",
      " 98%|█████████▊| 433953/443430 [00:00<00:00, 487221.77it/s]\u001b[A\n",
      "100%|██████████| 443430/443430 [00:00<00:00, 480707.33it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"pmacs/train_word_dict.csv\", 'w') as f:\n",
    "    output = csv.DictWriter(f, fieldnames=[\"Key\", \"Value\"])\n",
    "    output.writeheader()\n",
    "    for key in tqdm.tqdm(lstm.word_dict.d):\n",
    "        output.writerow({'Key':key, 'Value': lstm.word_dict.d[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 316 ms, total: 10.9 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dev_cands = session.query(DiseaseGene).filter(DiseaseGene.split == 1).order_by(DiseaseGene.id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 763802/763802 [33:11<00:00, 383.56it/s]  \n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('pmacs/dev_candidates_ends.csv', 'wb') as g:\n",
    "    with open(\"pmacs/dev_candidates_offsets.csv\", \"wb\") as f:\n",
    "        output = csv.writer(f)\n",
    "        for c in tqdm.tqdm(dev_cands):\n",
    "            data, ends = lstm._preprocess_data([c])\n",
    "            output.writerow(data[0])\n",
    "            g.write(\"{}\\n\".format(ends[0]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
