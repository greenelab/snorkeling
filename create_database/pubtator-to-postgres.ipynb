{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting Pubtator into a Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to load each [Pubmed](https://www.ncbi.nlm.nih.gov/pubmed/) abstract. It uses our own [pubtator repository](https://github.com/greenelab/pubtator) to convert each abstract into the appropriate format (xml) to load into a postgres database. Run the pubtator scripts before running this notebook, so the pubtator xml file can be constructed. Once constructed this notebook is designed to parse the data into a database. After loading each abstract, candidate extraction is performed for each relationship type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUST RUN AT THE START OF EVERYTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all necessary imports for the rest of this notebook. Plus, set up the postgres database for database operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:41:00.516124Z",
     "start_time": "2020-01-15T17:41:00.150405Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#Imports\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:41:00.724929Z",
     "start_time": "2020-01-15T17:41:00.517611Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "username = \"danich1\"\n",
    "password = \"snorkel\"\n",
    "dbname = \"pubmeddb\"\n",
    "\n",
    "#Path subject to change for different os\n",
    "database_str = \"postgresql+psycopg2://{}:{}@/{}?host=/var/run/postgresql\".format(username, password, dbname)\n",
    "os.environ['SNORKELDB'] = database_str\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:41:01.095678Z",
     "start_time": "2020-01-15T17:41:00.726737Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import PretaggedCandidateExtractor\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from snorkel.parser import DocPreprocessor\n",
    "from snorkel.models import Document\n",
    "\n",
    "from sqlalchemy import func\n",
    "from string import punctuation\n",
    "import lxml.etree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:41:01.120836Z",
     "start_time": "2020-01-15T17:41:01.097126Z"
    }
   },
   "outputs": [],
   "source": [
    "def offsets_to_token(left, right, offset_array, lemmas, punc=set(punctuation)):\n",
    "    \"\"\"Calculate the offset from tag to token\n",
    "    Ripped off from the snorkel custom tagger.\n",
    "    Designed to get the offset where a given token is found so it can receive a custom entity tag.\n",
    "    (i.e. Gene or Chemical)\n",
    "    Keyword arguments\n",
    "    left - the start of the tag\n",
    "    right - the end of the tag\n",
    "    offset_array - array of offsets given by stanford corenlp\n",
    "    lemmas - array of lemmas given by stanford corenlp\n",
    "    punc - a list of punctuation characters\n",
    "    \"\"\"\n",
    "    token_start, token_end = None, None\n",
    "    for i, c in enumerate(offset_array):\n",
    "        if left >= c:\n",
    "            token_start = i\n",
    "        if c > right and token_end is None:\n",
    "            token_end = i\n",
    "            break\n",
    "    token_end = len(offset_array) - 1 if token_end is None else token_end\n",
    "    token_end = token_end - 1 if lemmas[token_end - 1] in punc else token_end\n",
    "    return range(token_start, token_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:41:01.157761Z",
     "start_time": "2020-01-15T17:41:01.122133Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tagger(object):\n",
    "    \"\"\"Custom Tagger Class\n",
    "    This is a custom class that is designed to tag each relevant word\n",
    "    in a given sentence.\n",
    "    i.e if it sees GAD then this tagger will give GAD a Gene tag.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filter_df):\n",
    "        \"\"\" Initialize the tagger class\n",
    "        Keyword arguments:\n",
    "        self -- the class object\n",
    "        filter_df -- a pandas group object for quick indexing\n",
    "        \"\"\"\n",
    "        self.annt_df = filter_df\n",
    "\n",
    "    def tag(self, parts):\n",
    "        \"\"\"Tag each Sentence\n",
    "        Keyword arguments:\n",
    "        self -- the class object\n",
    "        parts -- standford's corenlp object which consists of nlp properties\n",
    "        Returns:\n",
    "        An updated parts object containing specified custom tags.\n",
    "        \"\"\"\n",
    "\n",
    "        pubmed_id, _, _, sent_start, sent_end = parts['stable_id'].split(':')\n",
    "        sent_start, sent_end = int(sent_start), int(sent_end)\n",
    "\n",
    "        # For each tag in the given document\n",
    "        # assign it to the correct word and move on\n",
    "        # if int(pubmed_id) not in self.annt_df['pubmed_id']:\n",
    "        #    return parts\n",
    "        try:\n",
    "            for index, tag in self.annt_df.get_group(int(pubmed_id)).iterrows():\n",
    "                if not (sent_start <= int(tag['offset']) <= sent_end):\n",
    "                    continue\n",
    "\n",
    "                offsets = [offset + sent_start for offset in parts['char_offsets']]\n",
    "                toks = offsets_to_token(int(tag['offset']), int(tag['end']), offsets, parts['lemmas'])\n",
    "                for tok in toks:\n",
    "                    parts['entity_types'][tok] = tag['type']\n",
    "                    parts['entity_cids'][tok] = tag['identifier']\n",
    "\n",
    "            return parts\n",
    "\n",
    "        except KeyError as e:\n",
    "            return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:41:01.192737Z",
     "start_time": "2020-01-15T17:41:01.162732Z"
    }
   },
   "outputs": [],
   "source": [
    "class XMLMultiDocPreprocessor(DocPreprocessor):\n",
    "    \"\"\"Hijacked this class to make it memory efficient\n",
    "        Fixes the main issue where crashes if GB files are introduced\n",
    "        et.iterparse for the win in memory efficiency\n",
    "    \"\"\"\n",
    "    def __init__(self, path, doc='.//document', text='./text/text()', id='./id/text()', tag_filter=None):\n",
    "        \"\"\"Initialize the XMLMultiDocPreprocessor Class\n",
    "        Keyword Arguments:\n",
    "        path - the absolute path of the xml file\n",
    "        doc - the xpath notation for document obejcts\n",
    "        test - the xpath for grabbing all the text objects\n",
    "        id - the xpath for grabbing all the id tags\n",
    "        \"\"\"\n",
    "        DocPreprocessor.__init__(self, path)\n",
    "        self.doc = doc\n",
    "        self.text = text\n",
    "        self.id = id\n",
    "        self.tag_filter = tag_filter\n",
    "\n",
    "    def parse_file(self, f, file_name):\n",
    "        \"\"\"This method overrides the original method\n",
    "        Keyword arguments:\n",
    "        f - the file object to be parsed by lxml\n",
    "        file_name - the name of the file used as metadata\n",
    "        Yields:\n",
    "        A document object in sqlalchemy format and the corresponding text\n",
    "        that will be parsed by CoreNLP\n",
    "        \"\"\"\n",
    "\n",
    "        for event, doc in et.iterparse(f, tag='document'):\n",
    "            doc_id = str(doc.xpath(self.id)[0])\n",
    "\n",
    "            if int(doc_id) not in self.tag_filter:\n",
    "                doc.clear()\n",
    "                continue\n",
    "\n",
    "            text = '\\n'.join(filter(lambda t: t is not None, doc.xpath(self.text)))\n",
    "\n",
    "            # guarentees that resources are freed after they have been used\n",
    "            doc.clear()\n",
    "            meta = {'file_name': str(file_name)}\n",
    "            stable_id = self.get_stable_id(doc_id)\n",
    "            if not(text):\n",
    "                continue\n",
    "\n",
    "            yield Document(name=doc_id, stable_id=stable_id, meta=meta), text\n",
    "\n",
    "    def _can_read(self, fpath):\n",
    "        \"\"\" Straight forward function\n",
    "        Keyword Arguments:\n",
    "        fpath - the absolute path of the file.\n",
    "        \"\"\"\n",
    "        return fpath.endswith('.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the Pubmed Abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is designed to read and parse data gathered from our [pubtator repo](https://github.com/greenelab/pubtator) (will refer as PubtatorR for clarity). PubtatorR is designed to gather and parse [NER](https://en.wikipedia.org/wiki/Named-entity_recognition) tagged Medline abstracts from NCBI's [PubTator](https://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/PubTator/). It outputs PubTator's annotated text in xml format, which is the standard format we are going to use. For this project, the id's are not from PubTator but from [Hetionet](https://think-lab.github.io/p/rephetio/). Since Pubtator contains over 10 million abstracts, the code below and in subsequent notebooks have been optimized to be memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:41:57.152425Z",
     "start_time": "2020-01-15T17:41:01.194697Z"
    }
   },
   "outputs": [],
   "source": [
    "filter_df = pd.read_table('https://github.com/greenelab/pubtator/raw/631e86002e11c41cfcfb0043e60b84ab321bdae3/data/pubtator-hetnet-tags.tsv.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:41:57.168495Z",
     "start_time": "2020-01-15T17:41:57.154609Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped = filter_df.groupby('pubmed_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:42:05.397514Z",
     "start_time": "2020-01-15T17:41:57.170198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please change to your local document here\n",
    "# Refer to https://github.com/greenelab/pubtator for instructions\n",
    "# to download and parse Pubtator\n",
    "working_path = '/home/danich1/Documents/Database/pubmed_docs.xml'\n",
    "xml_parser = XMLMultiDocPreprocessor(\n",
    "    path= working_path,\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()', tag_filter=set(filter_df['pubmed_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:42:05.415454Z",
     "start_time": "2020-01-15T17:42:05.398782Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dg_tagger = Tagger(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:42:26.517523Z",
     "start_time": "2020-01-15T17:42:05.416707Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus_parser = CorpusParser(fn=dg_tagger.tag)\n",
    "document_chunk = []\n",
    "\n",
    "for document in tqdm.tqdm(xml_parser.generate()):\n",
    "    \n",
    "    document_chunk.append(document)\n",
    "\n",
    "    # chunk the data because snorkel cannot \n",
    "    # scale properly\n",
    "    if len(document_chunk) >= 5e4:\n",
    "        corpus_parser.apply(document_chunk, parallelism=5, clear=False)\n",
    "        document_chunk = []\n",
    "    \n",
    "# If generator exhausts and there are still\n",
    "# document to parse\n",
    "if len(document_chunk) > 0:\n",
    "    corpus_parser.apply(data, parallelism=5, clear=False)\n",
    "    document_chunk = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get each candidate relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing the above abstracts, the next step in this pipeline is to extract candidates from all the tagged sentences. A candidate is considered a candidate if two mentions occur in the same sentence. For this pilot study, we are only considering the follow candidate relationships: Disease-Gene, Gene-Gene, Compound-Gene, Compound-Disease. In conjunction with extracting candidates, this part of the pipeline also stratifies each sentence into three different categories: Train (70%), Dev (20%), and Test (10%). These set categories will be used in subsequent notebooks ([3](3.data-gen-model.ipynb), [4](4.data-disc-model.ipynb), [5](5.data-analysis.ipynb)) for training and testing the machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:42:37.385469Z",
     "start_time": "2020-01-15T17:42:37.354055Z"
    }
   },
   "outputs": [],
   "source": [
    "chunk_size = 2e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:42:38.801013Z",
     "start_time": "2020-01-15T17:42:38.785784Z"
    }
   },
   "outputs": [],
   "source": [
    "def insert_cand_to_db(extractor, sentences):\n",
    "    for split, sens in enumerate(sentences):\n",
    "        extractor.apply(sens, split=split, parallelism=5, clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:42:39.465697Z",
     "start_time": "2020-01-15T17:42:39.431063Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_candidates(context_class, edge):\n",
    "    for i, label in enumerate([\"Train\", \"Dev\", \"Test\"]):\n",
    "        cand_len = session.query(context_class).filter(context_class.split == i).count()\n",
    "        print(\"Number of Candidates for {} edge and {} set: {}\".format(edge, label, cand_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:42:42.747010Z",
     "start_time": "2020-01-15T17:42:42.452447Z"
    }
   },
   "outputs": [],
   "source": [
    "#This specifies the type of candidates to extract\n",
    "DiseaseGene = candidate_subclass('DiseaseGene', ['Disease', 'Gene'])\n",
    "dge = PretaggedCandidateExtractor(DiseaseGene, ['Disease', 'Gene'])\n",
    "\n",
    "GeneGene = candidate_subclass('GeneGene', ['Gene1', 'Gene2'])\n",
    "gge = PretaggedCandidateExtractor(GeneGene, ['Gene', 'Gene'])\n",
    "\n",
    "CompoundGene = candidate_subclass('CompoundGene', ['Compound', 'Gene'])\n",
    "cge = PretaggedCandidateExtractor(CompoundGene, ['Compound', 'Gene'])\n",
    "\n",
    "CompoundDisease = candidate_subclass('CompoundDisease', ['Compound','Disease'])\n",
    "cde = PretaggedCandidateExtractor(CompoundDisease, ['Compound', 'Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:42:45.439265Z",
     "start_time": "2020-01-15T17:42:45.404945Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the seed for reproduction\n",
    "np.random.seed(100)\n",
    "total_sentences = session.execute(\"select count(*) from sentence\").fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:42:47.221112Z",
     "start_time": "2020-01-15T17:42:46.114744Z"
    }
   },
   "outputs": [],
   "source": [
    "category_list = np.random.choice([0,1,2], total_sentences, p=[0.7,0.2,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:43:14.441510Z",
     "start_time": "2020-01-15T17:43:11.615064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Divide the sentences into train, dev and test sets\n",
    "   \n",
    "#Grab the sentences!!!\n",
    "train_sens = set()\n",
    "dev_sens = set()\n",
    "test_sens = set()\n",
    "\n",
    "offset = 0\n",
    "category_index = 0\n",
    "sql_query = session.query(Document).limit(chunk_size)\n",
    "\n",
    "#divde and insert into the database\n",
    "while True:\n",
    "    documents = list(sql_query.offset(offset).all())\n",
    "    \n",
    "    if not documents:\n",
    "        break\n",
    "        \n",
    "    for doc in tqdm.tqdm(documents): \n",
    "        for s in doc.sentences:\n",
    "            \n",
    "            # Stratify the data into train, dev, test \n",
    "            category = category_list[category_index]\n",
    "            category_index = category_index + 1\n",
    "            \n",
    "            if category == 0:\n",
    "                train_sens.add(s)\n",
    "            elif category == 1:\n",
    "                dev_sens.add(s)\n",
    "            else:\n",
    "                test_sens.add(s)\n",
    "\n",
    "    # insert all the edge types\n",
    "    for edges in [dge, gge, cge, cde]:\n",
    "        insert_cand_to_db(edges, [train_sens, dev_sens, test_sens])\n",
    "        \n",
    "    offset = offset + chunk_size\n",
    "\n",
    "    #Reset for each chunk\n",
    "    train_sens = set()\n",
    "    dev_sens = set()\n",
    "    test_sens = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:43:16.774793Z",
     "start_time": "2020-01-15T17:43:16.719107Z"
    }
   },
   "outputs": [],
   "source": [
    "print_candidates(DiseaseGene, 'DiseaseGene')\n",
    "print_candidates(GeneGene, 'GeneGene')\n",
    "print_candidates(CompoundGene, 'CompoundGene')\n",
    "print_candidates(CompoundDisease, 'CompoundDisease')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the Potential Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one cool thing about jupyter is that you can use this tool to look at candidates. Check it out after everything above has finished running. The highlighted words are what Hetionet tagged as name entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:43:22.412690Z",
     "start_time": "2020-01-15T17:43:22.398228Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAINING_SET = 0\n",
    "DEVELOPMENT_SET = 1\n",
    "TEST_SET = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:46:31.468147Z",
     "start_time": "2020-01-15T17:46:31.411622Z"
    }
   },
   "outputs": [],
   "source": [
    "candidates = session.query(DiseaseGene).filter(DiseaseGene.split==TRAINING_SET).limit(100)\n",
    "sv = SentenceNgramViewer(candidates, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T17:46:33.106410Z",
     "start_time": "2020-01-15T17:46:33.067525Z"
    }
   },
   "outputs": [],
   "source": [
    "sv"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:snorkel_db]",
   "language": "python",
   "name": "conda-env-snorkel_db-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
